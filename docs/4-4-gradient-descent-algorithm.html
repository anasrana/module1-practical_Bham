<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="4.4 Gradient descent algorithm (+) | Essentials of Mathematics and Statistics" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://anasrana.github.io/module1-practical_Bham/" />

<meta property="og:description" content="This contains practicals for the second week of module 1 (Essentials of Mathematics and Statistics)." />
<meta name="github-repo" content="anasrana/module1-practical_Bham" />

<meta name="author" content="Anas A Rana" />

<meta name="date" content="2019-10-16" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="This contains practicals for the second week of module 1 (Essentials of Mathematics and Statistics).">

<title>4.4 Gradient descent algorithm (+) | Essentials of Mathematics and Statistics</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="1-1-prerequisites.html#prerequisites"><span class="toc-section-number">1.1</span> Prerequisites</a></li>
<li><a href="1-2-data-sets.html#data-sets"><span class="toc-section-number">1.2</span> Data sets</a></li>
</ul></li>
<li class="has-sub"><a href="2-practical-linear-regression.html#practical-linear-regression"><span class="toc-section-number">2</span> Practical: Linear regression</a><ul>
<li><a href="2-1-data.html#data"><span class="toc-section-number">2.1</span> Data</a></li>
<li><a href="2-2-simulating-data.html#simulating-data"><span class="toc-section-number">2.2</span> Simulating data</a></li>
<li class="has-sub"><a href="2-3-fitting-simple-linear-regression-model.html#fitting-simple-linear-regression-model"><span class="toc-section-number">2.3</span> Fitting simple linear regression model</a><ul>
<li><a href="2-3-fitting-simple-linear-regression-model.html#least-squared-estimation"><span class="toc-section-number">2.3.1</span> Least squared estimation</a></li>
<li><a href="2-3-fitting-simple-linear-regression-model.html#maximum-likelihood-estimation"><span class="toc-section-number">2.3.2</span> Maximum likelihood estimation</a></li>
</ul></li>
<li><a href="2-4-effect-of-variance.html#effect-of-variance"><span class="toc-section-number">2.4</span> Effect of variance</a></li>
<li class="has-sub"><a href="2-5-exercise.html#exercise"><span class="toc-section-number">2.5</span> Exercise</a><ul>
<li><a href="2-5-exercise.html#part-i"><span class="toc-section-number">2.5.1</span> Part I</a></li>
<li><a href="2-5-exercise.html#part-ii"><span class="toc-section-number">2.5.2</span> Part II</a></li>
<li><a href="2-5-exercise.html#part-iii"><span class="toc-section-number">2.5.3</span> Part III</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="model-answers-linear-regression.html#model-answers-linear-regression">Model answers: Linear regression</a><ul>
<li><a href="2-6-exercise-i.html#exercise-i"><span class="toc-section-number">2.6</span> Exercise I</a></li>
<li class="has-sub"><a href="2-7-exercise-ii.html#exercise-ii"><span class="toc-section-number">2.7</span> Exercise II</a><ul>
<li><a href="2-7-exercise-ii.html#comparison-of-data"><span class="toc-section-number">2.7.1</span> Comparison of data</a></li>
</ul></li>
<li><a href="2-8-exercise-ii-1.html#exercise-ii-1"><span class="toc-section-number">2.8</span> Exercise II</a></li>
</ul></li>
<li class="has-sub"><a href="3-practical-principal-component-analysis.html#practical-principal-component-analysis"><span class="toc-section-number">3</span> Practical: Principal component analysis</a><ul>
<li><a href="3-1-data-1.html#data-1"><span class="toc-section-number">3.1</span> Data</a></li>
<li><a href="3-2-introduction-1.html#introduction-1"><span class="toc-section-number">3.2</span> Introduction</a></li>
<li><a href="3-3-exercise-i-1.html#exercise-i-1"><span class="toc-section-number">3.3</span> Exercise I</a></li>
<li><a href="3-4-exercise-ii-2.html#exercise-ii-2"><span class="toc-section-number">3.4</span> Exercise II</a></li>
<li><a href="3-5-exercise-iii.html#exercise-iii"><span class="toc-section-number">3.5</span> Exercise III</a></li>
<li><a href="3-6-exercise-iv-single-cell-data.html#exercise-iv-single-cell-data"><span class="toc-section-number">3.6</span> Exercise IV: Single cell data</a></li>
</ul></li>
<li class="has-sub"><a href="4-practical-multiple-regression.html#practical-multiple-regression"><span class="toc-section-number">4</span> Practical: Multiple regression</a><ul>
<li><a href="4-1-multiple-regression.html#multiple-regression"><span class="toc-section-number">4.1</span> Multiple regression</a></li>
<li><a href="4-2-categorical-covariates.html#categorical-covariates"><span class="toc-section-number">4.2</span> Categorical covariates</a></li>
<li><a href="4-3-residuals.html#residuals"><span class="toc-section-number">4.3</span> Residuals</a></li>
<li><a href="4-4-gradient-descent-algorithm.html#gradient-descent-algorithm"><span class="toc-section-number">4.4</span> Gradient descent algorithm (+)</a></li>
</ul></li>
<li class="has-sub"><a href="5-practical-generalised-linear-models.html#practical-generalised-linear-models"><span class="toc-section-number">5</span> Practical: Generalised linear models</a><ul>
<li><a href="5-1-data-2.html#data-2"><span class="toc-section-number">5.1</span> Data</a></li>
<li><a href="5-2-detecting-snp-associations.html#detecting-snp-associations"><span class="toc-section-number">5.2</span> Detecting SNP associations</a></li>
<li><a href="5-3-gwas-and-logistic-regression.html#gwas-and-logistic-regression"><span class="toc-section-number">5.3</span> GWAS and logistic regression</a></li>
<li class="has-sub"><a href="5-4-negative-binomial-and-poisson-regression.html#negative-binomial-and-poisson-regression"><span class="toc-section-number">5.4</span> Negative binomial and Poisson regression</a><ul>
<li><a href="5-4-negative-binomial-and-poisson-regression.html#count-based-glms"><span class="toc-section-number">5.4.1</span> Count-based GLMs</a></li>
<li><a href="5-4-negative-binomial-and-poisson-regression.html#fitting-a-glm"><span class="toc-section-number">5.4.2</span> Fitting a GLM</a></li>
<li><a href="5-4-negative-binomial-and-poisson-regression.html#comparing-nested-models"><span class="toc-section-number">5.4.3</span> Comparing nested models</a></li>
</ul></li>
<li><a href="5-5-negative-binomial-vs-poisson-glms.html#negative-binomial-vs-poisson-glms"><span class="toc-section-number">5.5</span> Negative-Binomial vs Poisson GLMs</a></li>
<li><a href="5-6-further-understanding-the-model-optional.html#further-understanding-the-model-optional"><span class="toc-section-number">5.6</span> Further understanding the model (<strong>OPTIONAL</strong>)</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="gradient-descent-algorithm" class="section level2">
<h2><span class="header-section-number">4.4</span> Gradient descent algorithm (+)</h2>
<p>Finally, in todays practical we will implement the <em>gradient descent algorithm</em> which we discussed in the lecture.</p>
<p>For simplicity we will only consider the case with one covariate. In this section we will use simulated data and compare the results with <code>lm()</code>. The model we will simulate from is:</p>
<p><span class="math display">\[y = 2 + 3 x + \epsilon\]</span></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="4-4-gradient-descent-algorithm.html#cb88-1"></a><span class="co"># setting seed to be able to reproduce the simulation</span></span>
<span id="cb88-2"><a href="4-4-gradient-descent-algorithm.html#cb88-2"></a><span class="kw">set.seed</span>(<span class="dv">200</span>)</span>
<span id="cb88-3"><a href="4-4-gradient-descent-algorithm.html#cb88-3"></a></span>
<span id="cb88-4"><a href="4-4-gradient-descent-algorithm.html#cb88-4"></a><span class="co"># number of samples</span></span>
<span id="cb88-5"><a href="4-4-gradient-descent-algorithm.html#cb88-5"></a>n_sample &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb88-6"><a href="4-4-gradient-descent-algorithm.html#cb88-6"></a></span>
<span id="cb88-7"><a href="4-4-gradient-descent-algorithm.html#cb88-7"></a><span class="co"># We sample x values from a uniform distribution in the range [-5, 5]</span></span>
<span id="cb88-8"><a href="4-4-gradient-descent-algorithm.html#cb88-8"></a>x &lt;-<span class="st"> </span><span class="kw">runif</span>(n_sample, <span class="dv">-5</span>, <span class="dv">5</span>)</span>
<span id="cb88-9"><a href="4-4-gradient-descent-algorithm.html#cb88-9"></a><span class="co"># Next we compute y</span></span>
<span id="cb88-10"><a href="4-4-gradient-descent-algorithm.html#cb88-10"></a>y &lt;-<span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n_sample, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span>
<span id="cb88-11"><a href="4-4-gradient-descent-algorithm.html#cb88-11"></a></span>
<span id="cb88-12"><a href="4-4-gradient-descent-algorithm.html#cb88-12"></a>sim_df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)</span>
<span id="cb88-13"><a href="4-4-gradient-descent-algorithm.html#cb88-13"></a></span>
<span id="cb88-14"><a href="4-4-gradient-descent-algorithm.html#cb88-14"></a><span class="kw">ggplot</span>(sim_df, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span></span>
<span id="cb88-15"><a href="4-4-gradient-descent-algorithm.html#cb88-15"></a><span class="st">    </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="Practicals-module_1_files/figure-html/sim-mr-1.png" width="2100" /></p>
<p>Recall that in gradient descent we want to minimise the Mean Squared Error (<span class="math inline">\(J(\beta)\)</span>) which is the cost function. The first step is to write this cost function in R. For simplicity we will use matrix multiplication, which in R is implemented as <code>%*%</code>. (<em>Note</em>, to get help on these function with special characters you can’t simply run the command <code>?%*%</code> instead you have to put it in quotes <code>?"%*%"</code>.)</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="4-4-gradient-descent-algorithm.html#cb89-1"></a>cost_fn &lt;-<span class="st"> </span><span class="cf">function</span>(X, y, coef) {</span>
<span id="cb89-2"><a href="4-4-gradient-descent-algorithm.html#cb89-2"></a>    <span class="kw">sum</span>( (X <span class="op">%*%</span><span class="st"> </span>coef <span class="op">-</span><span class="st"> </span>y)<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span><span class="kw">length</span>(y))</span>
<span id="cb89-3"><a href="4-4-gradient-descent-algorithm.html#cb89-3"></a>}</span></code></pre></div>
<p>To perform an optimisation we will have to initialise parameters, in general optimisation algorithms won’t always produce the same results for all choices of initialisations.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="4-4-gradient-descent-algorithm.html#cb90-1"></a><span class="co"># First we set alpha and the number of iterations we will perform</span></span>
<span id="cb90-2"><a href="4-4-gradient-descent-algorithm.html#cb90-2"></a>alpha &lt;-<span class="st"> </span><span class="fl">0.2</span></span>
<span id="cb90-3"><a href="4-4-gradient-descent-algorithm.html#cb90-3"></a>num_iters &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb90-4"><a href="4-4-gradient-descent-algorithm.html#cb90-4"></a></span>
<span id="cb90-5"><a href="4-4-gradient-descent-algorithm.html#cb90-5"></a><span class="co"># next we will initialise regression coefficients</span></span>
<span id="cb90-6"><a href="4-4-gradient-descent-algorithm.html#cb90-6"></a>coef &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">nrow=</span><span class="dv">2</span>)</span>
<span id="cb90-7"><a href="4-4-gradient-descent-algorithm.html#cb90-7"></a>X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">matrix</span>(x))</span>
<span id="cb90-8"><a href="4-4-gradient-descent-algorithm.html#cb90-8"></a>res &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;list&quot;</span>, num_iters)</span></code></pre></div>
<p>We now write a for loop to compute the optimisation, where we store the full history of the opmtimisation.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="4-4-gradient-descent-algorithm.html#cb91-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>num_iters) {</span>
<span id="cb91-2"><a href="4-4-gradient-descent-algorithm.html#cb91-2"></a>  error &lt;-<span class="st"> </span>(X <span class="op">%*%</span><span class="st"> </span>coef <span class="op">-</span><span class="st"> </span>y)</span>
<span id="cb91-3"><a href="4-4-gradient-descent-algorithm.html#cb91-3"></a>  delta &lt;-<span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>error <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(y)</span>
<span id="cb91-4"><a href="4-4-gradient-descent-algorithm.html#cb91-4"></a>  coef &lt;-<span class="st"> </span>coef <span class="op">-</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span>delta</span>
<span id="cb91-5"><a href="4-4-gradient-descent-algorithm.html#cb91-5"></a>  res_df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">itr =</span> i , <span class="dt">cost =</span> <span class="kw">cost_fn</span>(X, y, coef),</span>
<span id="cb91-6"><a href="4-4-gradient-descent-algorithm.html#cb91-6"></a>                   <span class="dt">b0 =</span> coef[<span class="dv">1</span>], <span class="dt">b1 =</span> coef[<span class="dv">2</span>])</span>
<span id="cb91-7"><a href="4-4-gradient-descent-algorithm.html#cb91-7"></a></span>
<span id="cb91-8"><a href="4-4-gradient-descent-algorithm.html#cb91-8"></a>  res[[i]] &lt;-<span class="st"> </span>res_df</span>
<span id="cb91-9"><a href="4-4-gradient-descent-algorithm.html#cb91-9"></a>}</span></code></pre></div>
<p>We created a list to store results <code>res</code> it is possible to combine all results into a simple <code>data.frame</code> using the <code>bind_rows()</code> function from the <code>dplyr</code> package. If we look at the final values in the resulting variable we will</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="4-4-gradient-descent-algorithm.html#cb92-1"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb92-2"><a href="4-4-gradient-descent-algorithm.html#cb92-2"></a>res_df &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(res)</span>
<span id="cb92-3"><a href="4-4-gradient-descent-algorithm.html#cb92-3"></a><span class="kw">tail</span>(res_df)</span></code></pre></div>
<pre><code>##     itr      cost       b0       b1
## 95   95 0.5275707 2.034285 3.014512
## 96   96 0.5275707 2.034285 3.014512
## 97   97 0.5275707 2.034285 3.014512
## 98   98 0.5275707 2.034285 3.014512
## 99   99 0.5275707 2.034285 3.014512
## 100 100 0.5275707 2.034285 3.014512</code></pre>
<p>We can see that <span class="math inline">\(\beta_0 = 2\)</span> and <span class="math inline">\(\beta_1 = 3\)</span> are reproduced faithfully. There are a few ways to visualise the optimisation. We can look at the convergence of the parameters, the cost function itself or even the estimated <span class="math inline">\(y\)</span> at each step of the optimisation.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="4-4-gradient-descent-algorithm.html#cb94-1"></a><span class="kw">ggplot</span>(res_df, <span class="kw">aes</span>(<span class="dt">x =</span> itr, <span class="dt">y =</span> b1)) <span class="op">+</span></span>
<span id="cb94-2"><a href="4-4-gradient-descent-algorithm.html#cb94-2"></a><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb94-3"><a href="4-4-gradient-descent-algorithm.html#cb94-3"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Iteration&quot;</span>,</span>
<span id="cb94-4"><a href="4-4-gradient-descent-algorithm.html#cb94-4"></a>         <span class="dt">y =</span> <span class="st">&quot;Estimated beta_1&quot;</span>,</span>
<span id="cb94-5"><a href="4-4-gradient-descent-algorithm.html#cb94-5"></a>         <span class="dt">title =</span> <span class="st">&quot;Visuaslisation of the cconvergence of the beta_1 parameter&quot;</span>)</span></code></pre></div>
<p><img src="Practicals-module_1_files/figure-html/vis_gd-1.png" width="3000" /></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="4-4-gradient-descent-algorithm.html#cb95-1"></a><span class="kw">ggplot</span>(res_df, <span class="kw">aes</span>(<span class="dt">x =</span> itr, <span class="dt">y =</span> cost)) <span class="op">+</span></span>
<span id="cb95-2"><a href="4-4-gradient-descent-algorithm.html#cb95-2"></a><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb95-3"><a href="4-4-gradient-descent-algorithm.html#cb95-3"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Iteration&quot;</span>,</span>
<span id="cb95-4"><a href="4-4-gradient-descent-algorithm.html#cb95-4"></a>         <span class="dt">y =</span> <span class="st">&quot;Cost function&quot;</span>,</span>
<span id="cb95-5"><a href="4-4-gradient-descent-algorithm.html#cb95-5"></a>         <span class="dt">title =</span> <span class="st">&quot;History of cost function at each iteration&quot;</span>)</span></code></pre></div>
<p><img src="Practicals-module_1_files/figure-html/vis_gd-2.png" width="3000" /></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="4-4-gradient-descent-algorithm.html#cb96-1"></a><span class="kw">ggplot</span>(sim_df, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span></span>
<span id="cb96-2"><a href="4-4-gradient-descent-algorithm.html#cb96-2"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb96-3"><a href="4-4-gradient-descent-algorithm.html#cb96-3"></a><span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">data =</span> res_df, <span class="kw">aes</span>(<span class="dt">intercept =</span> b0, <span class="dt">slope =</span> b1),</span>
<span id="cb96-4"><a href="4-4-gradient-descent-algorithm.html#cb96-4"></a>                <span class="dt">alpha =</span> <span class="fl">0.3</span>, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="dt">size =</span> <span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb96-5"><a href="4-4-gradient-descent-algorithm.html#cb96-5"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;y&quot;</span>,</span>
<span id="cb96-6"><a href="4-4-gradient-descent-algorithm.html#cb96-6"></a>         <span class="dt">title =</span> <span class="st">&quot;Estimated response at each step during optimisation&quot;</span>)</span></code></pre></div>
<p><img src="Practicals-module_1_files/figure-html/vis_gd-3.png" width="3000" /></p>
<p>Now compare these results to the ones obtained by fitting a linear model in R using the function <code>lm()</code>, how different are the results. Try to reproduce these plots with <span class="math inline">\(\alpha =\)</span> (0.02, 0.1, 0.5), and different number of iterations in the optimisation and compare the estimated <span class="math inline">\(\hat{\beta}_0\)</span>, and <span class="math inline">\(\hat{\beta}_1\)</span> to the values you use during the simulation step. This will give you an idea how important the right choice of these two parameters is.</p>

</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="4-3-residuals.html"><button class="btn btn-default">Previous</button></a>
<a href="5-practical-generalised-linear-models.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
