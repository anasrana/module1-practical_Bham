[
["index.html", "Essentials of Mathematics and Statistics Practical: Module 1 Chapter 1 Introduction 1.1 Prerequisites", " Essentials of Mathematics and Statistics Practical: Module 1 Anas A Rana 2019-10-04 Chapter 1 Introduction This is part of the practical for Module 1 - Essentials of Mathematics and Statistics part of the MSc Bioinformatics 2018/19 at the University of Birmingham. This website hosts the practiclas for week two of Module 1, which covers: Linear regression Principal Component Analysis (PCA) Multivariate Regression Generalised Linear Models 1.1 Prerequisites Ensure you have attended Module 1 lectures and have the installed R and/or Rstudio. Rstudio is recommended, any packages requried for each practical are mentioned at the beginning of each practical. "],
["practical-linear-regression.html", "Chapter 2 Practical: Linear Regression 2.1 Simulating data 2.2 Fitting simple linear regression model 2.3 Effect of variance 2.4 Exercise", " Chapter 2 Practical: Linear Regression In this practical you will go through some of the basics of linear modelling in R as well as simulating data. The practical contains the following elements: simulate linear regression model investigate parameters characterise prediction accurary correlation of real world data We will use reshape2, ggplo2, and bbmle pcakges. Run the following command to make sure they are installed and loaded install.packages(&quot;ggplot2&quot;) install.packages(&quot;reshape2&quot;) install.packages(&quot;bbmle&quot;) library(ggplot2) library(reshape2) library(bbmle) 2.1 Simulating data You will simulate data based on the simple linear regression model: \\[ y_i = \\beta_0 + \\beta_1\\, x_i + \\epsilon_i, \\] where \\((x_i, y_i)\\) represent the \\(i\\)-th measurement pair with \\(i = 1, \\ldots, N\\), \\(\\beta_0\\) and \\(\\beta_1\\) are regression coefficients representing intercept and slope respectively. We assume the noise term \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) is normally distributed with zero mean and variance \\(\\sigma^2\\). First we define the values of the parameters of linear regression \\((\\beta_0, \\beta_1, \\sigma^2)\\): b0 &lt;- 10 # regression coefficient for intercept b1 &lt;- -8 # regression coefficient for slope sigma2 &lt;- 0.5 # noise variance In the next step we will simulate \\(N = 100\\) covariates \\(x_i\\) by randomly sampling from the standard normal distribution: set.seed(198) # set a seed to ensure data is reproducible N &lt;- 100 # no of data points to simulate x &lt;- rnorm(N, mean = 0, sd = 1) # simulate covariate Next we simulate the error term: # simulate the noise terms, rnorm requires the standard deviation e &lt;- rnorm(N, mean = 0, sd = sqrt(sigma2)) Finally we have all the parameters and variables to simulate the response variable \\(y\\): # compute (simulate) the response variable y = b0 + b1 * x + e We will plot our data using ggplot2 so the data need to be in a data.frame object: # Set up the data point sim_data &lt;- data.frame(x = x, y = y) # create a new scatter plot using ggplot2 ggplot(sim_data, aes(x = x, y = y)) + geom_point() We define the true data y_true to be the true linear relationship between the covariate and the response without the noise. # Compute true y values y_true &lt;- b0 + b1 * x # Add the data to the existing data frame sim_data$y_true &lt;- y_true Now we will add the true values of \\(y\\) to the scatter plot: lr_plot &lt;- ggplot(sim_data, aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = y_true), colour = &quot;red&quot;) print(lr_plot) 2.2 Fitting simple linear regression model 2.2.1 Least squared estimation Now that you have simulated data you can use it to regress \\(y\\) on \\(x\\), since this is simulated data we know the parameters and can make a comparison. In R we can use the function lm() for this, by default it implements a least squares estimate: # Use the lm function to fit the data ls_fit &lt;- lm(y ~ x, data = sim_data) # Display a summary of fit summary(ls_fit) ## ## Call: ## lm(formula = y ~ x, data = sim_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.69905 -0.41534 0.02851 0.41265 1.53651 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.95698 0.06701 148.6 &lt;2e-16 *** ## x -7.94702 0.07417 -107.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6701 on 98 degrees of freedom ## Multiple R-squared: 0.9915, Adjusted R-squared: 0.9914 ## F-statistic: 1.148e+04 on 1 and 98 DF, p-value: &lt; 2.2e-16 The output for lm() is an object (in this case ls_fit) which contains multiple variables. To access them there are some built in functions, e.g. coef(), residuals(), and fitted(). We will explore these in turn: # Extract coefficients as a named vector ls_coef &lt;- coef(ls_fit) print(ls_coef) ## (Intercept) x ## 9.956981 -7.947016 # Extract intercept and slope b0_hat &lt;- ls_coef[1] # alternative ls_fit$coefficients[1] b1_hat &lt;- ls_coef[2] # alternative ls_fit$coefficients[2] # Generate the predicted data based on estimated parameters y_hat &lt;- b0_hat + b1_hat * x sim_data$y_hat &lt;- y_hat # add to the existing data frame # Create scatter plot and lines for the original and fitted lr_plot &lt;- ggplot(sim_data, aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = y_true), colour = &quot;red&quot;, size = 1.3) + # plot predicted relationship in blue geom_line(aes(x = x, y = y_hat), colour = &quot;blue&quot;) # force Rstudio to display the plot print(lr_plot) The estimated parameters and the plot shows a good correspondence between fitted regression parameters and the true relationship between \\(y\\) and \\(x\\). We can check this by plotting the residuals, this data is stored as the residuals parameter in the ls_fit object. # Residuals ls_residual &lt;- residuals(ls_fit) # can also be accessed via ls_fit$residuals # scatter plot of residuals plot(ls_residual) A better way of summarising the data is to visualise them as a histogram: hist(ls_residual) We expect the mean and variance of the residuals to be close to the level used to generate the data. print(mean(ls_residual)) ## [1] -7.157903e-18 print(var(ls_residual)) ## [1] 0.4444955 This is as expected since subtracting a good fit from the data leaves \\(\\epsilon\\) which has \\(0\\) mean and \\(0.5\\) variance. 2.2.2 Maximum likelihood estimation Next you will look at maximum likelihood estimation based on the same data you simulated earlier. This is a bit more involved as it requires you to explicitly write the function you wish to minimise. The function we use is part of the bbmle package. # Loading the required package library(bbmle) # function that will be minimised. It takes as arguments all parameters # Here we are helped by the way R works we don&#39;t have to explicitly pass x. # The function will use the existing estimates in the environment mle_ll &lt;- function(beta0, beta1, sigma) { # first we predict the response variable based on the guess for our response y_pred = beta0 + beta1 * x # next we calculate the normal distribution based on the predicted value # the guess for sigma and return the log log_lh &lt;- dnorm(y, mean = y_pred, sd = sigma, log = TRUE) # We returnr the negative sum of the log likelihood return(-sum(log_lh)) } # This is the function that actually performs the estimation # The first variable here is the function we will use # The second variable passed is a list of initial guesses of parameters mle_fit &lt;- mle2(mle_ll, start = list(beta0 = -1, beta1 = 20, sigma = 10)) # With the same summary function as above we can output a summary of the fit summary(mle_fit) ## Maximum likelihood estimation ## ## Call: ## mle2(minuslogl = mle_ll, start = list(beta0 = -1, beta1 = 20, ## sigma = 10)) ## ## Coefficients: ## Estimate Std. Error z value Pr(z) ## beta0 9.957019 0.066336 150.099 &lt; 2.2e-16 *** ## beta1 -7.947005 0.073426 -108.231 &lt; 2.2e-16 *** ## sigma 0.663347 0.046904 14.143 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## -2 log L: 201.7011 The estimated parameters using the maximum likelihood are also a very good estimate of the true values. 2.3 Effect of variance Now investigate the quality of the predictions further by simulating more data sets and seeing how the variance affects the quality of the fit as indicated by the mean-squared error (mse). To start you will define some parameter for the simulations, the number of simulations to run for each variance, and the variance values to try. # number of simulations for each noise level n_simulations &lt;- 100 # A vector of noise levels to try sigma_v &lt;- c(0.1, 0.4, 1.0, 2.0, 4.0, 6.0, 8.0) n_sigma &lt;- length(sigma_v) # Create a matrix to store results mse_matrix &lt;- matrix(0, nrow = n_simulations, ncol = n_sigma) # name row and column rownames(mse_matrix) &lt;- c(1:n_simulations) colnames(mse_matrix) &lt;- sigma_v Next we will write a nested for loop. The first loop will be over the variances and a second loop over the number of repeats. We will simulate the data, perform a fit with lm(). We can use the fitted() function on the resulting object to extract the fitted values \\(\\hat{y}\\) and use this to compute the mean-squared error from the true value \\(y\\). # loop over variance for (i in 1:n_sigma) { sigma2 &lt;- sigma_v[i] # for each simulation for (it in 1:n_simulations) { # Simulate the data x &lt;- rnorm(N, mean = 0, sd = 1) e &lt;- rnorm(N, mean = 0, sd = sqrt(sigma2)) y &lt;- b0 + b1 * x + e # set up a data frame and run lm() sim_data &lt;- data.frame(x = x, y = y) lm_fit &lt;- lm(y ~ x, data = sim_data) # compute the mean squared error between the fit and the actual y&#39;s y_hat &lt;- fitted(lm_fit) mse_matrix[it, i] &lt;- mean((y_hat - y)^2) } } We created a matrix to store the mse values, but to plot them using ggplot2 we have to convert them to a data.frame. This can be done using the melt() function form the reshape2 library. We can compare the results using boxplots. library(reshape2) # convert the matrix into a data frame for ggplot2 mse_df &lt;- melt(mse_matrix) # rename the columns names(mse_df) &lt;- c(&quot;Simulation&quot;, &quot;variance&quot;, &quot;MSE&quot;) # now use a boxplot to look at the relationship between # mean-squared prediction error and variance mse_plt &lt;- ggplot(mse_df, aes(x = variance, y = MSE)) + geom_boxplot(aes(group = variance)) print(mse_plt) You can see that the variances of the mse and the value of the mse go up with increasing variance in the simulation. What changes do you need to make to the above function to plot the accuracy of the estimated regression coefficients as a function of variance? 2.4 Exercise 2.4.1 Part I Read in the data in storks.txt, compute the correlation and comment on it. The data represents no of storks (column 1) in Oldenburg Germany from \\(1930 - 1939\\) and the number of people (column 2). 2.4.2 Part II Fit a simple linear model to the two data sets supplied (lr_data1.Rdata and lr_data2.Rdata). In both files the \\((x,y)\\) data is saved in two vectors, \\(x\\) and \\(y\\). Download the data from Canvas, you can read it into R and plot it with the following commands: load(&quot;lr_data1.Rdata&quot;) plot(x, y) load(&quot;lr_data2.Rdata&quot;) plot(x, y) Fit the linear model and comment on the differences between the data. 2.4.3 Part III Investigate how the sample size will affect the quality of the fit using mse, use the code for investigating the affect of variance as inspiration. "],
["model-answers-linear-regression.html", "Model answers: Linear Regression 2.5 Excercise I 2.6 Excersise II 2.7 Excersise II", " Model answers: Linear Regression 2.5 Excercise I library(ggplot2) library(reshape2) stork_dat &lt;- read.table(&quot;stork.txt&quot;, hedaer = TRUE) ggplot(stork_dat, aes(x = no_storks, y = people)) + geom_point(size = 2) This is a plot of number of people in Oldenburg (Germany) against the number of storks. We can calculate the correlation in R cor(stork_dat$no_storks, stork_dat$peopl) ## [1] 0.9443965 This is a very high correlation, and obviously there is no causation. Think about why there would be a correlation between these two random variables. 2.6 Excersise II # load first data set and create data.frame load(&quot;lr_data1.Rdata&quot;) sim_data1 &lt;- data.frame(x = x, y = y) # load second data set and create data.frame load(&quot;lr_data2.Rdata&quot;) sim_data2 &lt;- data.frame(x = x, y = y) lr_fit1 &lt;- lm(y ~ x, data = sim_data1) lr_fit2 &lt;- lm(y ~ x, data = sim_data2) 2.6.1 Comparison of data ggplot(sim_data1, aes(x = x, y = y)) + geom_point(size = 1.5) + geom_point(data = sim_data2, color = &quot;red&quot;, shape = 18) If we plot the data on tope of each other, the first data set in black and the second one in red, we can see a small number of points are different between the two data sets. summary(lr_fit1) ## ## Call: ## lm(formula = y ~ x, data = sim_data1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8309 -0.6910 0.0296 0.7559 3.3703 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -20.1876 0.1250 -161.46 &lt;2e-16 *** ## x 2.8426 0.1138 24.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.229 on 98 degrees of freedom ## Multiple R-squared: 0.8643, Adjusted R-squared: 0.8629 ## F-statistic: 624.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 summary(lr_fit2) ## ## Call: ## lm(formula = y ~ x, data = sim_data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.386 -1.960 -1.084 -0.206 54.516 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -18.9486 0.8006 -23.669 &lt; 2e-16 *** ## x 2.1620 0.7285 2.968 0.00377 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.87 on 98 degrees of freedom ## Multiple R-squared: 0.08245, Adjusted R-squared: 0.07309 ## F-statistic: 8.806 on 1 and 98 DF, p-value: 0.003772 From the summary data we can see a discrepancy between the two estimates in the regression coefficients (\\(\\approx 1\\)), though the error in the estimate is quite large. The other thing to notice is that the summary of the residuals look quite different. If we investigate further and plot them we se: plot(residuals(lr_fit1)) plot(residuals(lr_fit2)) Here we can once again see the outliers in the second data set which affect the estimation. We now plot the histogram and boxplots for comparison: hist(residuals(lr_fit1)) hist(residuals(lr_fit2)) boxplot(residuals(lr_fit2), residuals(lr_fit1)) Her we can see that the distribution of the residuals has significantly changed in data set 2. A change in only 4 data points was sufficient to change the regression coefficients. 2.7 Excersise II b0 &lt;- 10 # regression coefficient for intercept b1 &lt;- -8 # regression coefficient for slope sigma2 &lt;- 0.5 # noise variance # number of simulations for each sample size n_simulations &lt;- 100 # A vector of sample sizes to try sample_size_v &lt;- c( 5, 20, 40, 80, 100, 150, 200, 300, 500, 750, 1000 ) n_sample_size &lt;- length(sample_size_v) # Create a matrix to store results mse_matrix &lt;- matrix(0, nrow = n_simulations, ncol = n_sample_size) # name row and column rownames(mse_matrix) &lt;- c(1:n_simulations) colnames(mse_matrix) &lt;- sample_size_v # loop over sample size for (i in 1:n_sample_size) { N &lt;- sample_size_v[i] # for each simulation for (it in 1:n_simulations) { x &lt;- rnorm(N, mean = 0, sd = 1) e &lt;- rnorm(N, mean = 0, sd = sqrt(sigma2)) y &lt;- b0 + b1 * x + e # set up a data frame and run lm() sim_data &lt;- data.frame(x = x, y = y) lm_fit &lt;- lm(y ~ x, data = sim_data) # compute the mean squared error between the fit and the actual y&#39;s y_hat &lt;- fitted(lm_fit) mse_matrix[it, i] &lt;- mean((y_hat - y)^2) } } library(reshape2) mse_df = melt(mse_matrix) # convert the matrix into a data frame for ggplot names(mse_df) = c(&quot;Simulation&quot;, &quot;Sample_Size&quot;, &quot;MSE&quot;) # rename the columns # now use a boxplot to look at the relationship between mean-squared prediction error and sample size mse_plt = ggplot(mse_df, aes(x=Sample_Size, y=MSE)) mse_plt = mse_plt + geom_boxplot( aes(group=Sample_Size) ) print(mse_plt) You should see that the variance of the mean-squared error goes down as the sample size goes up and converges towards a limiting value. Larger sample sizes help reduce the variance in our estimators but do not make the estimates more accurate. Can you do something similar to work out the relationship between how accurate the regression coefficient estimates are as a function of sample size? "]
]
