---
title: "Essentials of Mathematics and Statistics"
author: "Anas A Rana"
date: "`r Sys.Date()`"
bibliography:
- book.bib
description: This contains practicals for Module 1 (_Essentials of Mathematics and Statistics_).
documentcalss: book
output:
    bookdown::gitbook:
        lib_dir: assets
        split_by: chapter
    bookdown::pdf_book:
        keep_tex: no
link-citations: yes
site: bookdown::bookdown_site
subtitle: 'Practical: Module 1'
biblio-style: apalike
url: 'https\://anasrana.github.io/module1-practical_Bham/'
github-repo: "anasrana/module1-practical_Bham"
---

# Introduction

This is part of the practical for Module 1 - Essentials of Mathematics and Statistics part of the [MSc Bioinformatics](https://www.birmingham.ac.uk/postgraduate/courses/taught/med/bioinformatics.aspx) at the University of Birmingham.

This website hosts all practicals for **Module 1**, which covers:

- Linear regression
- Principal Component Analysis (PCA)
- Multivariate Regression
- Generalised Linear Models

## How to use these resources

Section 2 covers some basic concepts on using `R` and `Rstudio`. You should have already covered it, but this is a refresher.

Section 3 contains a link to one of the locations you can download the data from.

The sections after that contain the content of the practical we will go through.

# Getting started in R and Rstudio

You have already started working with `R`, here are just some of the core principles revisited. Ensure you understand them as they are required knowledge and will not be revisited during the module.

## R scripts

While entering and running your code at the R command line is effective and simple. This technique has its limitations. Each time you want to execute a set of commands, you have to re-enter them at the command line. Complex commands are potentially subject to typographical errors, necessitating that they be re-entered correctly. Repeating a set of operations requires re-entering the code stream. Fortunately, R and RStudio provide a method to mitigate these issues. R scripts are that solution. A script is simply a text file containing a set of commands and comments. The script can be saved and used later to re-execute the saved commands. The script can also be edited so you can execute a modified version of the commands.

## Creating an R script

It is easy to create a new script in RStudio. You can open a new empty script by clicking the New File icon in the upper left of the main RStudio toolbar. This icon looks like a white square with a white plus sign in a green circle. Clicking the icon opens the New File Menu. Click the R Script menu option and the script editor will open with an empty script.

```{r, echo = FALSE}
knitr::include_graphics("images/rs-new-file.png")
```

Once the new script opens in the Script Editor panel, the script is ready for text entry, and your RStudio session will look like this.

```{r, echo = FALSE}
knitr::include_graphics("images/rs-file-open.png")
```

Here is an easy example to familiarize you with the Script Editor interface. Type the following code into your new script *(later topics will explain the specific code components do)*.

```{r eval=F}
    # this is my first R script
    # do some things
    x = 34
    y = 16
    z = x + y   # addition
    w = y/x     # division
    # display the results
    x
    y
    z
    w
    # change x
    x = "some text"
    # display the results
    x
    y
    z
    w
```

```{r, echo = FALSE}
knitr::include_graphics("images/rs-file-content.png")
```

There, you now have your first R script. Notice how the editor places a number in front of each line of code. The line numbers can be helpful as you work with your code. Before proceeding on to executing this code, it would be a good idea to learn how to save your script.

## Saving an R script

You can save your script by clicking on the Save icon at the top of the Script Editor panel. When you do that, a Save File dialog will open.

```{r, echo = FALSE}
knitr::include_graphics("images/rs-save.png")
```

## Executing code in an R script

You can run the code in your R script easily. The Run button in the Script Editor panel toolbar will run either the current line of code or any block of selected code. You can use your First script.R code to gain familiarity with this functionality.

Place the cursor anywhere in line 3 of your script \[x = 34\]. Now press the Run button in the Script Editor panel toolbar. Three things happen: 1) the code is transferred to the command console, 2) the code is executed, and 3) the cursor moves to the next line in your script. Press the Run button three more times. RStudio executes lines 4, 5, and 6 of your script.

Now you will run a set of code commands all at once. Highlight lines 8, 9, 10, and 11 in the script.

```{r, echo = FALSE}
knitr::include_graphics("images/rs-run.png")
```

Highlighting is accomplished similar to what you may be familiar with in word processor applications. You click your left mouse button and the beginning of the text you want to highlight, you hold the mouse button and drag the cursor to the end of the text and release the button. With those four lines of code highlighted, click the editor Run button. All four lines of code are executed in the command console. That is all it takes to run script code in RStudio.

### Comments in an R script (documenting your code)

Before finishing this topic, there is one final concept you should understand. It is always a good idea to place comments in your code. They will help you understand what your code is meant to do. This will become helpful when you reopen code you wrote weeks ago and are trying to work with again. The saying, "Real programmers do not document their code. If it was hard to write, it should be hard to understand" is meant to be a dark joke, not a coding style guide.

```{r, echo = FALSE}
knitr::include_graphics("images/rs-comment.png")
```

A comment in R code begins with the `#` symbol. Your code in First `script.R` contains several examples of comments. Lines `1`, `2`, `7`, `12`, and `14` in the image above are all comment lines. Any line of text that starts with `#` will be treated as a comment and will be ignored during code execution. Lines `5` and `6` in this image contain comments at the end. All text after the `#` is treated as a comment and is ignored during execution.

Notice how the RStudio editor shows these comments colored green. The green color helps you focus on the code and not get confused by the comments.

Besides using comments to help make your `R` code more easily understood, you can use the `#` symbol to ignore lines of code while you are developing your code stream. Simply place a `#` in front of any line that you want to ignore. `R` will treat those lines as comments and ignore them. When you want to include those lines again in the code execution, remove the `#` symbols and the code is executable again. This technique allows you to change what code you execute without having to retype deleted code.

# Data sets

For each practical there are some datasets required, you will find all data required for week two of practicals in the data folder [here](https://github.com/anasrana/module1-practical_Bham/tree/master/data). Links to individual datasets requried can be found at the beginning of each practical or on Canvas.

<!--chapter:end:index.Rmd-->

# Simulating random numbers

There are a number of functions in `R` that you can use to simulate random numbers according to different probability distributions.

The function `sample` allows you to take a sample of the specified size from the elements of a vector `x`using sampling with or without replacement. You can use `?sample` to read the documentation describing the command.

In the following, we will use the `sample` function to make 10,000 draws from the set of numbers `1`, `2`, `3` and `4` and display the distribution of the sampled values using a histogram.

First, we define a vector called `x` which contains the numbers `1`, `2`, `3`, and `4`. The function `c` allows us to combine these four numbers together into one vector:

```{r}
x <- c(1, 2, 3, 4)
```


We now use the function `sample` to pick from those four numbers in `x` 10,000 times. The result, the 10,000 numbers chosen, is store in `out`:

```{r}
out <- sample(x, 10000, replace=TRUE)
```

Lets plot a histogram of the values picked:

```{r}
hist_out <- hist(out, main = '', xlab = 'Values', ylab = 'Frequency')
```

We picked each number with equal probability so the histogram shows each number is equally likely to have been chosen.

**Problem**

> What is the difference in the output `out1` and `out2` in the following piece of code?

```{r}
x <- c( 1, 2, 2, 3, 4, 1, 6, 7, 8, 10, 5, 5, 1, 4, 9 )
out1 <- sample(x, 10, replace=FALSE)
out2 <- sample(x, 10, replace=TRUE)
```

The option `replace=TRUE` activates sampling with replacement (i.e. the numbers that are picked are put back and can be picked again).

The option `replace=FALSE` activates sampling without replacement (i.e. the numbers that are picked are not put back and cannot be picked again).

**Problem**

> Use the `sample` or `sample.int` function to simulate values from rolls of an unbiased six-sided die. Show that the distribution of values you obtain is consistent with an unbiased die.

*Hint 1*: Type `?sample.int` in the console to get help on this function.

*Hint 2*: You may find it useful to use the function `table`. Type `?table` in the console to get help on this function.

```{r}
rolls_from_sample = sample(c(1:6), size=5000, replace=TRUE)
rolls_from_sample.int = sample(6, size=5000, replace=TRUE)

table(rolls_from_sample)
```

```{r}
table(rolls_from_sample.int)
```

Both gives a uniform distribution over the numbers 1-6. The function `sample.int` is a specialised version of sample for sampling integers. Many `R` libraries have specialised versions of more general functions to do specific tasks under certain conditions.

# Markov Chains

<!-- In this practical you will learn a number of techniques in R that you will use to simulate simplified games of Monopoly (<https://en.wikipedia.org/wiki/Monopoly_(game)>). In addition, there are also many tutorials and guides on the Web describing how to produce computer simulations for Monopoly. You are welcome to read and use these examples to inspire your work. -->

We will now look at a **Markov Chain**. We have not covered it during lectures but based on the basic principles we have covered we will be able to use it for simulations.

Any random process is known to have the *Markov property* (a Markov process) if the probability of going to the next state depends only on the current state and not on the past states. A Markov process is **memoryless property** in that it does not store any property or memory of its past states.

If a Markov process operates within a specific (finite) set of states, it is called a ***Markov Chain***.

A Markov Chain is defined by three properties:

1.  A state space: a set of values or states in which a process could exist

2.  A transition matrix: defines the probability of moving from one state to another state

3.  A current state probability distribution: defines the probability of being in any one of the states at the start of the process

Consider the following example where we have two states describing the weather on any particular day: (i) Sunny and (ii) Rainy. Each arrow denotes the probability of going from one state to itself or another over the course of a day. For example, if it is currently sunny, the probability of it raining the next day is 0.6. Conversely, if it is raining, the probability that it will become sunny the next day is 0.7 and 0.3 that it will continue raining.

```{r, echo=FALSE, engine='tikz', out.width='90%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', engine.opts = list(template = "latex/tikz2pdf.tex")}
\begin{tikzpicture}[node distance=2cm,->,>=latex,auto,
  every edge/.append style={thick}]
  \node[state] (1) {Sunny};
  \node[state] (2) [right of=1] {Rainy};
  \path (1) edge[loop left]  node{$0.4$} (1)
            edge[bend left]  node{$0.6$}   (2)
        (2) edge[loop right] node{$0.3$}  (2)
            edge[bend left] node{$0.7$}     (1);
\end{tikzpicture}
```

The transition matrix can be written as the following in R:

```{r}
transitionMatrix = matrix(c(0.4, 0.6, 0.7, 0.3), nrow=2, ncol=2, byrow=TRUE)
print(transitionMatrix)
```

which creates a 2 x 2 matrix consisting of the transition probabilities shown in the diagram.

Suppose I want to simulate a sequence of 30 days and the weather patterns over those days. Assuming that on day 0 it is currently sunny, I can do the following:

```{r}
# initial state - it is [1] sunny or [2] rainy
state <- 1
weather_sequence <- rep(0, 30) # vector to store simulated values
for (day in 1:30) { # simulate for 30 days
  pr <- transitionMatrix[state, ] # select the row of transition probabilities

  # sample [1] or [2] based on the probs pr
  state = sample(c(1, 2), size = 1, prob = pr)
  weather_sequence[day] <- state # store the sampled state
}

# print the simulated weather sequence
print(weather_sequence)
```


**Problem**

> Can you extend this example to a three-state model?

```{r, echo=FALSE, engine='tikz', out.width='90%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', engine.opts = list(template = "latex/tikz2pdf.tex")}
\begin{tikzpicture}[node distance=4cm,->,>=latex,auto,
  every edge/.append style={thick}]

  \node[state] (1) {Cloudy};
   \node[state] (2) [below left of =1] {Sunny};
  \node[state] (3) [below right of=1] {Rainy};


  \path (1) edge[loop above]  node{$?$} (1)
            edge[bend left =12]  node{$0.6$}   (2)
            edge[bend left = 12] node{$ 0.2$} (3)
        (2) edge[loop below] node{$?$}  (2)
            edge[bend left=12] node{$0.1$}     (1)
            edge[bend right = 12] node {$0.2$} (3)
        (3) edge[loop below] node{$?$} (3)
            edge[bend left = 15] node{$0.4$} (1)
            edge[bend right = 15] node{$0.3$} (2);
\end{tikzpicture}
```

Note, the diagram (intentionally) misses out the self-transitions. You should be able to infer this because the probabilities given would otherwise not add up to one!

# Model answers 3 state Markov Chain{-}

Set up a 3x3 transition matrix:

```{r}
transitionMatrix = matrix(c(0.7, 0.2, 0.1,
                            0.3, 0.3, 0.4,
                            0.6, 0.2, 0.2), nrow=3, ncol=3, byrow=TRUE)

# Check matrix set-up correctly
print(transitionMatrix)

```

Note the ordering of the states is arbitrary but here we have used the convention that State 1 is Sunny, State 2 is Rainy and State 3 is Cloudy which means the probabilities are completed in that order in the transition matrix. We just need to be consistent.

```{r}
state <- 1 # initial state - it is [1] sunny, [2] rainy and [3] cloudy
weather_sequence <- rep(0, 30) # vector to store simulated values

# simulate for 30 days
for (day in 1:30) {
  pr <- transitionMatrix[state, ] # select the row of transition probabilities

  # sample [1-3] based on the probs pr
  state <- sample(c(1, 2, 3), size = 1, prob = pr)
  weather_sequence[day] <- state # store the sampled state
}
print(weather_sequence)
```

# A Monopoly simulation

Now you will use to simulate simplified games of Monopoly (https://en.wikipedia.org/wiki/Monopoly_(game)). In addition, there are also many tutorials and guides on the Web describing how to produce computer simulations for Monopoly. You are welcome to read and use these examples to inspire your work.

## Moving around the board

A Monopoly board has 40 spaces. Players take it in turns to roll two dice and traverse around the board according to the sum of the dice values.

Use the following code example to simulate turns of a single player:

```{r}
num_turns <- 100000 # number of turns to take

current_board_position <- 0 # start on the GO space

move_size <- rep(0, num_turns)
positions_visited <- rep(0, num_turns)

# use a for loop to simulate a number of turns
for (turn in 1:num_turns) {

  # roll two dice
  die_values <- sample(c(1:6), 2, replace = TRUE)

  # move player position

  # number of positions to move
  plus_move <- sum(die_values)

  # compute new board position
  new_board_position <- current_board_position + plus_move

  # update board position (this corrects for the fact the board is circular)
  current_board_position <- (new_board_position %% 40)

  # store position visited
  positions_visited[turn] <- current_board_position

}

```


By increasing the number of turns taken, what distribution does the set of simulated board positions converge towards? Show this graphically using the histogram function.

```{r}
hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)
```

## Going to Jail

If a player lands on to Go To Jail space they must move immediately to the Jail space. Extend your code to include the possibility of going to jail. Here, assume that once in jail, the player continues as normal on the next turn.

```{r}
 num_turns <- 100000 # number of turns to take

current_board_position <- 0 # start on the GO space
go_to_jail_position <- 30 # the go to jail space
jail_position <- 10 # jail space

move_size <- rep(0, num_turns)
positions_visited <- rep(0, num_turns)

# use a for loop to simulate a number of turns
for (turn in 1:num_turns) {

  # roll two dice
  die_values <- sample(c(1:6), 2, replace = TRUE)

  # move player position

  # number of positions to move
  plus_move <- sum(die_values)

  # compute new board position
  new_board_position <- current_board_position + plus_move

  # if land on GO TO JAIL square, then go backwards to the JAIL square
  if (new_board_position == go_to_jail_position) {
    new_board_position <- jail_position
  }

  # update board position (this corrects for the fact the board is circular)
  current_board_position <- (new_board_position %% 40)

  # store position visited
  positions_visited[turn] <- current_board_position

}
```

> What is the distribution of board positions during a long game?

```{r}
hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)
```




> Can you explain this result qualitatively?

You can also go to jail, if you roll three doubles (both dice having the same value) in a row. Update your code to allow for the possibility of going to Jail with three doubles. How does the distribution of board positions change?

```{r}
num_turns <- 100000 # number of turns to take

current_board_position <- 0 # start on the GO space
go_to_jail_position  <- 30 # the go to jail space
jail_position <- 10 # jail space

move_size <- rep(0, num_turns)
positions_visited <- rep(0, num_turns)

# use a for loop to simulate a number of turns
for (turn in 1:num_turns) {

  # set double counter to zero
  double_counter <- 0

  # roll (max) three times
  for (j in 1:3){

    # roll two dice
    die_values <- sample(c(1:6), 2, replace = TRUE)

    # if we have rolled a double for the third time, we proceed straight to jail
    if ((die_values[1] == die_values[2]) & (double_counter == 2 )) {
      current_board_position <- jail_position
      break
    }

    # otherwise

    # move player position

    # number of positions to move
    plus_move <- sum(die_values)

    # compute new board position
    new_board_position <- current_board_position + plus_move

    # if land on GO TO JAIL square, then go backwards to the JAIL square
    if (new_board_position == go_to_jail_position) {
      new_board_position <- jail_position
    }

    # update board position (this corrects for the fact the board is circular)
    current_board_position <- (new_board_position %% 40)

    # break out of loop if we roll a non-double
    if (die_values[1] != die_values[2]) {
      break
    } else { # increment double counter
      double_counter <- double_counter + 1
    }

  }

  # store final position visited
  positions_visited[turn] <- current_board_position


}


hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)
```

Adding the rolling doubles feature doesn't seem to change much. We might expect this since rolling three doubles is a very unlikely event!

## Further Exercises

Now consider building a more complex Monopoly simulation by incorporating more complex aspects of the game such as:

-   the purchase of properties
-   a ledger for each player
-   chance and community cards

You will need to think carefully about the simplifying assumptions you will make to make the task achievable. Do not be over-ambitious. For example, you might initially assume that players will not build houses/hotels on properties.

Here are some questions to answer with your simulations:

1.  How many turns does it take before all properties are purchased?
2.  What are the best properties to buy?
3.  How long does it take for a winner to be determined?

For example, the following simple extension of the previous example adds some features to record properties being purchased. This simulation is constructed based on the assumption that a players always buys any free property that land on.

```{r}
num_games <- 1000 # number of games to play
num_turns <- 1000 # number of turns to take

current_board_position <- 0 # start on the GO space
go_to_jail_position <- 30 # the go to jail space
jail_position <- 10 # jail space
# vector of squares containing properties
properties_that_can_be_bought <- c(1, 3, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16,
   18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 37, 39)


# vector to store number of turns to buy all properties
time_to_buy_all_properties <- rep(0, num_games)

# simulate multiple games
for (game in 1:num_games) {

  positions_visited <- rep(0, num_turns)
  positions_purchased <- rep(0, 40)
  properties_bought <- rep(0, num_turns)

  # use a for loop to simulate a number of turns
  for (turn in 1:num_turns) {

    # roll two dice
    die_values <- sample(c(1:6), 2, replace = TRUE)

    # move player position

    # number of positions to move
    plus_move <- sum(die_values)

    # compute new board position
    new_board_position <- current_board_position + plus_move

    # if land on GO TO JAIL square, then go backwards to the JAIL square
    if (new_board_position == go_to_jail_position) {
      new_board_position <- jail_position
    }

    # update board position (this corrects for the fact the board is circular)
    current_board_position <- (new_board_position %% 40)

    # if we can on a square that can be purchased and which has not been
    # purchased (note R uses 1-indexing for arrays)
    if (positions_purchased[current_board_position+1] == 0) {
      if (current_board_position %in% properties_that_can_be_bought) {
        positions_purchased[current_board_position + 1] <- 1
      }
    }

    # store position visited
    positions_visited[turn] <- current_board_position

    # store number of properties bought
    properties_bought[turn] <- sum(positions_purchased)

    # check if all properties are gone
    if (properties_bought[turn] == length(properties_that_can_be_bought)) {
      time_to_buy_all_properties[game] <- turn
      break
    }


  }

}

hist(time_to_buy_all_properties, breaks = 20)
```

<!--chapter:end:01-markov-chain.Rmd-->

# Monte Carlo Methods

Monte Carlo (MC) simulations provide a means to model a problem and apply brute force computational power to achieve a solution - randomly simulate from a model until you get an answer. The best way to explain is to just run through a bunch of examples, so lets go!

## Integration

We will start with basic integration. Suppose we have an instance of a Normal distribution with a mean of 1 and a standard deviation of 2 then we want to find the *integral* (area under the curve) from 1 to 3:

\[
 \int_1^3 \frac{1}{10 \sqrt{2\,\pi}}\, e^{- \frac{(x - 1)^2}{2\times 2^2}}dx
\]

which we can visualise as follows:

```{r, echo=FALSE, engine='tikz', out.width='90%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', engine.opts = list(template = "latex/tikz2pdf.tex")}

\begin{tikzpicture}
\begin{axis}[domain=0:10, samples=100,
axis lines*=left, xlabel=\(x\), ylabel=\(p(x)\), title={N$(1, 2)$},
height=6cm, width=10cm,
xtick={-5,  0, 1, 3, 5}, ytick=\empty,
enlargelimits=false, clip=false, axis on top,
grid = major]
\addplot [fill=cyan!20, draw=none, domain=1:3] {gauss(1,2)} \closedcycle;
\addplot [very thick,cyan!50!black, domain=-7:9] {gauss(1,2)};
\end{axis}
\end{tikzpicture}

```

If you have not done calculus before - do not worry. We are going to write a Monte Carlo approach for estimating this integral which does not require any knowledge of calculus!

The method relies on being able to generate samples from this distribution and counting how many values fall between 1 and 3. The proportion of samples that fall in this range over the total number of samples gives the area.

First, create a new `R` script in `Rstudio`. Next we define the number of samples we will obtain. Lets choose 10,000

```{r}
n <- 100 # number of samples to take
```

Now we use the `R` function `rnorm` to simulate 100 numbers from a Normal distribution with mean 1 and standard deviation 2:

```{r}
sims <- rnorm(n, mean = 1, sd = 2) # simulated normally distributed numbers
```

Lets estimate the integral between 1 and 3 by counting how many samples had a value in this range:

```{r}
# find proportion of values between 1-3
mc_integral <- sum(sims >= 1 & sims <= 3) / n
```
The result we get is:

```{r}
print(mc_integral)
```

The exact answer given using the cumulative distribution function `pnorm` in R is given by:

```{r}
mc_exact = pnorm(q=3, mean=1, sd=2) - pnorm(q=1, mean=1, sd=2)
print(mc_exact)
```

The `pnorm` gives the integral under the Normal distribution (in this case with mean 1 and standard deviation 2) from negative infinity up to the value specified by `q`.

The first call to `pnorm(q=3, mean=1, sd=2)` gives us this integral:

```{r, echo=FALSE, engine='tikz', out.width='90%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', engine.opts = list(template = "latex/tikz2pdf.tex")}

\begin{tikzpicture}
\begin{axis}[domain=0:10, samples=100,
axis lines*=left, xlabel=\(x\), ylabel=\(p(x)\), title={N$(1, 2)$},
height=6cm, width=10cm,
xtick={-5,  0, 3, 5}, ytick=\empty,
enlargelimits=false, clip=false, axis on top,
grid = major]
\addplot [fill=cyan!20, draw=none, domain=-7:3] {gauss(1,2)} \closedcycle;
\addplot [very thick,cyan!50!black, domain=-7:9] {gauss(1,2)};
\end{axis}
\end{tikzpicture}

```

The second call to `pnorm(q=1, mean=1, sd=2)` gives us this integral:

```{r, echo=FALSE, engine='tikz', out.width='90%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', engine.opts = list(template = "latex/tikz2pdf.tex")}

\begin{tikzpicture}
\begin{axis}[domain=0:10, samples=100,
axis lines*=left, xlabel=\(x\), ylabel=\(p(x)\), title={N$(1, 2)$},
height=6cm, width=10cm,
xtick={-5,  0, 1, 5}, ytick=\empty,
enlargelimits=false, clip=false, axis on top,
grid = major]
\addplot [fill=cyan!20, draw=none, domain=-7:1] {gauss(1,2)} \closedcycle;
\addplot [very thick,cyan!50!black, domain=-7:9] {gauss(1,2)};
\end{axis}
\end{tikzpicture}

```

Therefore the difference between these gives us the integral of interest.

**The Monte Carlo estimate is a fairly good approximation to the true value!**

## Problem: MC accuracy

> 1. Try increasing the number of simulations and see how the accuracy improves?
> 2. Can you draw a graph of number of MC samples vs accuracy?

*Model answers are in the next section*

## Approximating the Binomial Distribution

We flip a coin 10 times and we want to know the probability of getting more than 3 heads. This is a trivial problem using the Binomial distribution but suppose we have forgotten about this or never learned it in the first place.

Lets solve this problem with a Monte Carlo simulation. We will use the common trick of representing tails with 0 and heads with 1, then simulate 10 coin tosses 100 times and see how often that happens.

```{r}
runs <- 100 # number of simulations to run

greater_than_three <- rep(0, runs) # vector to hold outcomes

# run 100 simulations
for (i in 1:runs) {

  # flip a coin ten times (0 - tail, 1 - head)
  coin_flips <- sample(c(0, 1), 10, replace = T)

  # count how many heads and check if greater than 3
  greater_than_three[i] <- (sum(coin_flips) > 3)
}

# compute average over simulations
pr_greater_than_three <- sum(greater_than_three) / runs
```

For our MC estimate of the probability \(P(X>3)\) we get

```{r}
print(pr_greater_than_three)
```

which we can compare to R’s built-in Binomial distribution function:

```{r}
print(pbinom(3, 10, 0.5, lower.tail = FALSE))
```

## Problem: MC Binomial

> 1. Try increasing the number of simulations and see how the accuracy improves?
> 2. Can you plot how the accuracy varies as a function of the number of simulations? (hint: see the previous section)



Not bad! **The Monte Carlo estimate is close to the true value.**

## Monte Carlo Expectations

Consider the following spinner. If the spinner is spun randomly then it has a probability 0.5 of landing on yellow and 0.25 of landing on red or blue respectively.

```{r, echo=FALSE, engine='tikz', out.width='20%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', engine.opts = list(template = "latex/tikz2pdf.tex")}

 \begin{tikzpicture}
    \pie[color={orange!50,blue!70,red!70}, sum=auto, before number=\myfrac, after number=\relax,text=white, font=\bfseries]
{{1/2}/,{1/4}/,{1/4}/}

   \end{tikzpicture}
```

If the rules of the game are such that landing on ‘yellow’ you gain 1 point, ‘red’ you lose 1 point and ‘blue’ you gain 2 points. We can easily calculate the expected score.

Let \(X\) denote the random variable associated with the score of the spin then:

\[
    E[X] = \frac{1}{2} \times 1 + \frac{1}{4} \times (-1) + \frac{1}{4} \times 2 = 0.75
\]

If we ask a more challenging question such as:

> **After 20 spins what is the probability that you will have less then 0 points?"**

How might we solve this?

Of course, there are methods to analytically solve this type of problem but by the time they are even explained we could have already written our simulation!

To solve this with a Monte Carlo simulation you need to sample from the Spinner 20 times, and return 1 if we are below 0 other wise we’ll return 0. We will repeat this 10,000 times to see how often it happens!

## Using Functions

First, we are going to introduce the concept of a function. This is a piece of code which is encapsulated so then we can refer to it repeated via the name of the function rather than repeatedly writing those lines of code. The function we will write will simulate one game as indicated above and return whether the number of points is less than zero.

```{r}
# simulates a game of 20 spins
play_game <- function(){
    # picks a number from the list (1, -1, 2)
    # with probability 50%, 25% and 25% twenty times
  results <- sample(c(1, -1, 2), 20, replace = TRUE, prob = c(0.5, 0.25, 0.25))

  # function returns whether the sum of all the spins is < 1
  return(sum(results) < 0)
}
```

## Simulating from function

Now we can use this function in a loop to play the game 100 times:

```{r}
runs <- 100 # play the game 100 times

less_than_zero <- rep(0, runs) # vector to store outcome of each game
for (it in 1:runs) {
  # play the game by calling the function and store the outcome
  less_than_zero[it] <- play_game()
}
```

We can then compute the probability that, after twenty spins, we will have less than zero points:

```{r}
prob_less_than_zero <- sum(less_than_zero)/runs
print(prob_less_than_zero)

```

The probability is very low. This is not surprising since there is only a 25% chance of getting a point deduction on any spin and a 75% chance of gaining points. Try to increase the number of simulation runs to see if you can detect any games where you do find a negative score.

## Problem: MC Expectation

> 1. Modify your code to allow you to calculate the expected number of points after 20 spins.
> 2. Simulate a game in which you have a maximum of 20 spins but you go “bust” once you hit a negative score and take this into account when you compute the expected end of game score.


# Model Answers: Monte Carlo {-}

## Problem: MC accuracy

First let's increase the number of simulations and out the accuracy

```{r}
sample_sizes <- c(10, 50, 100, 250, 500, 1000) # try different sample sizes
n_sample_sizes <- length(sample_sizes) # number of sample sizes to try
rpts <- 100 # number of repeats for each sample size
accuracy <- rep(0, n_sample_sizes) # vector to record accuracy values
accuracy_sd <- rep(0, n_sample_sizes) # vector to record accuracy sd values

# for each sample size
for (i in 1:n_sample_sizes) {

  sample_sz <- sample_sizes[i] # select a sanmple size to use

  # vector to store results from each repeat
  mc_integral <- rep(0, rpts)
  for (j in 1:rpts){
    # simulated normally distributed numbers
    sims <- rnorm(sample_sz, mean = 1, sd = 2)
    # find proportion of values between 1-3
    mc_integral[j] <- sum(sims >= 1 & sims <= 3) / sample_sz
  }

  # compute average difference between integral estimate and real value
  accuracy[i] <- mean(mc_integral - mc_exact)
  # compute sd difference between integral estimate and real value
  accuracy_sd[i] <- sd(mc_integral - mc_exact)

}

print(accuracy)

print(accuracy_sd)
print(accuracy + accuracy_sd)
```

Next, we will plot the results. Here we will make use of `ggplot2` a library to create nice plots without much effort. The input need to be a `data.frame` so we will need to create one based on the data.

```{r}
# load ggplot
library(ggplot2)

# create a data frame for plotting
df <- data.frame(sample_sizes, accuracy, accuracy_sd)

print(df)

# use ggplot to plot lines for the mean accuracy and error bars
# using the std dev
ggplot(df, aes(x = sample_sizes, y = accuracy)) +
  geom_line() +
  geom_point() +
  geom_errorbar(
      aes(ymin = accuracy - accuracy_sd, ymax = accuracy + accuracy_sd),
          width = .2,
          position = position_dodge(0.05)) +
  ylab("Estimate-Exact") +
  xlab("Run")
```

This shows that as the number of Monte Carlo samples is increased, the accuracy increases (i.e. the difference between the estimated integral value and real values converges to zero). In addition, the variability in the integral estimates across different simulation runs reduces.

## Problem: MC Expectation

### Problem 1

```{r}
# simulates a game of 20 spins
play_game <- function() {
    # picks a number from the list (1, -1, 2)
    #  with probability 50%, 25% and 25% twenty times
  results <- sample(c(1, -1, 2), 20, replace = TRUE, prob = c(0.5, 0.25, 0.25))
  return(sum(results)) # function returns the sum of all the spins
}

score_per_game = rep(0, runs) # vector to store outcome of each game
for (it in 1:runs) {
  score_per_game[it] <- play_game() # play the game by calling the function
}
expected_score = mean(score_per_game) # average over all simulations

print(expected_score)
```

### Problem 2

```{r}
# simulates a game of up to 20 spins
play_game <- function() {
    # picks a number from the list (1, -1, 2)
    #  with probability 50%, 25% and 25% twenty times
  results <- sample(c(1, -1, 2), 20, replace = TRUE, prob = c(0.5, 0.25, 0.25))
  results_sum <- cumsum(results) # compute a running sum of points
  # check if the game goes to zero at any point
  if (sum(results_sum <= 0)) {
    return(0) # return zero
  } else {
    return(results_sum[20]) # returns the final score
  }
}

game_score <- rep(0, runs) # vector to store scores in each game played

# for each game
for (it in 1:runs) {
  game_score[it] <- play_game()
}

print(mean(game_score))

plot(game_score)
```

The games with score zero now corresponds to the number of games where we went bust (or genuinely ended the game with zero).

# Maximum Likelihood

During the lectures, you saw how we could use a brute-force search of parameters to find the maximum likelihood estimate of an unknown mean for a Normal distribution given a set of data. In this exercise, we will now look at how we would do this more efficiently in real life.

## The likelihood function

First, we are going to write a function to compute the log-likelihood function given parameters:
```{r}
neglogLikelihood <- function(mu, x) {
  logF = dnorm(x, mean = mu, sd = 1, log = TRUE)
  return(-sum(logF))
}
```

Note that this function returns the `-sum(logF)` because the numerical optimisation algorithm we are going to use finds the *minimum* of a function. We are interested in the *maximum* likelihood but we can turn this into a minimisation problem by simply negating the likelihood.

Now, lets assume our data is captured in the following vector:

```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)
n = length(x)

```

## Optimisation

Now, we will need to define an initial search value for the parameter, we will arbitrarily pick a value:
```{r}
mu_init = 1.0
```
Now we will use the `R` function `optim` to find the maximum likelihood estimate. As mentioned above, `optim` finds the minimum value of a function so in this case we are trying to find the parameter that minimises the negative log likelihood.

```{r}
out <- optim(mu_init, neglogLikelihood, gr = NULL, x, method = "L-BFGS-B",
         lower = -Inf, upper = Inf)
```

Here, this says that we will start the search at `mu_init` using the function `logLikelihood` that we have defined above. The `optim` algorithm will use the `L-BFGS-B` search method. The parameter is allowed to take any value from `lower = -Inf` to `upper = Inf`. The result is stored in out.

Once the optimiser has run, we can see what parameter value it has found:

```{r}
print(out$par)
```

which we can compare against the sample mean

```{r}
print(mean(x))
```

It turns out that it is theoretically known that the maximum likelihood estimate, for this particular problem, is the sample mean which is why they coincide!

We can visualise this further. First we define an array of possible values for `mu` in this case between -0.1 and 0.3 with 101 values in-between:

```{r}
mu <- seq(-0.1, 0.3, length = 101)
```

We use the `apply` function to apply the `logLikelihood` function to each of the `mu` values we have defined. This means we do not need to use a for loop:

```{r}
neglogL <- apply( matrix(mu), 1, neglogLikelihood, x)
```

We can then plot and overlay the maximum likelihood result:

```{r}
plot(mu, neglogL, pch="-")
points(out$par, out$value, col="red", pch=0)
```

The plot shows that `optim` has found the `mu` which minimises the negative log-likelihood.

## Two-parameter estimation


Now suppose both the mean and the variance of the Normal distribution are unknown and we need to search over two parameters for the maximum likelihood estimation.

We now need a modified negative log-likelihood function:

```{r}
neglogLikelihood2 <- function(theta,x) {
  mu <- theta[1] # get value for mu
  sigma2 <- theta[2] # get value for sigma2

  # compute density for each data element in x
  logF <- dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE)

  return(-sum(logF)) # return negative log-likelihood
}
```

Notice that we pass through one argument `theta` whose elements are the parameters for `mu` and `sigma2` which we unpack within the function.

Now we can run `optim` but this time the initial parameters values must be initialised with two values. Furthermore, as variance cannot be negative, we bound the possible lower values that `sigma2` can take by setting `lower = c(-Inf, 0.001)`. The second argument means `sigma2` cannot be lower than 0.001:

```{r}
theta_init = c(1, 1)

out <- optim(theta_init, neglogLikelihood2, gr = NULL, x, method = "L-BFGS-B",
        lower = c(-Inf, 0.001), upper = c(Inf, Inf))
```

We can now visualise the results by creating a two-dimensional contour plot. We first need to generate a grid of values for `mu` and `sigma2`:

```{r}
# one dimensional grid of values for mu
mu <- seq(-0.1, 1.0, length = 101)
# one dimensional grid of values for sigma2
sigma2 <- seq(0.1, 1.0, length = 101)

mu_xx <- rep(mu, each = 101) # replicate this 101 times
sigma2_yy <- rep(sigma2, times = 101) # replicate this 101 times

# generate grid of values (each row contains a unique combination
# of mu and sigma2 values)
mu_sigma_grid <- cbind(mu_xx, sigma2_yy)
```

Now we apply our negative log-likehood function to this grid to generate a negative log-likelihood value for each position on the grid:

```{r}
neglogL2 <- apply(mu_sigma_grid, 1, neglogLikelihood2, x)
```

We now use the `contour` function to plot our results:

```{r}
# convert vector of negative log-likelihood values into a grid
neglogL2 <- matrix(neglogL2, 101)

# draw contour plot
contour(sigma2, mu, neglogL2, nlevels = 50, xlab = "sigma2", ylab = "mu")
# overlay the maximum likelihood estimate as a red circle
points(out$par[2], out$par[1], col="red")
```

Excellent! We have now found the maximum likelihood estimates for the unknown mean and variance for the Normal distribution that our data is assumed to be drawn from. Let’s compare our estimates against the sample mean and variance. First, the estimates:

```{r}
print(out$par[1]) # mu estimate
print(out$par[2]) # sigma2 estimate

```

Now, the sample mean and variances:

```{r}
print(mean(x)) # sample mean
print(var(x)) # sample variance (normalised by n-1)
print(var(x)*(n-1)/n) # sample variance (normalised by n)

```

Interesting! The maximum likelihood estimates return the sample mean and the **biased* sample variance estimate (where we normalise by \(n\)
 and not \(n−1\)). Indeed, it turns out that theoretically, the maximum likelihood estimate does give a biased estimate of the population variance.

## Problem: MLE

 > A potentially biased coin is tossed 10 times and the number of heads recorded. The experiment is repeated 5 times and the number of heads recorded was 3, 2, 4, 5 and 2 respectively.
> Can you derive a maximum likelihood estimate of the probability of obtaining a head?

# Model Answers: MLE {-}

```{r}
neglogLikelihood <- function(p, n, x) {
  # compute density for each data element in x
  logF <- dbinom(x, n, prob = c(p, 1 - p), log = TRUE)
  return(-sum(logF)) # return negative log-likelihood
}

n <- 10 # number of coin tosses
x <- c(3, 2, 4, 5, 2) # number of heads observed

p_init <- 0.5 # initial value of the probability

# run optim to get maximum likelihood estimates
out <- optim(p_init, neglogLikelihood, gr = NULL, n, x, method = "L-BFGS-B",
  lower = 0.001, upper = 1-0.001)

# create a grid of probability values
p_vals <- seq(0.001, 1 - 0.001, length = 101)

# use apply to compute the negative log-likelihood for each probability value
neglogL <- apply(matrix(p_vals), 1, neglogLikelihood, n, x)

# plot negative log-likelihood function and overlay maximum (negative)
# log-likelihood estimate
plot(p_vals, neglogL, pch = "-")
points(out$par, out$value, col = "red", pch = 0)
```

<!--chapter:end:02-mc.Rmd-->

# Confidence Intervals

In this practical, you will learn how to derive confidence intervals for a particular problem using Monte Carlo simulations.

In a random experiment, a sample of data is collected from which we can estimate a population parameter of interest. This estimate can either be a point estimate or an *interval estimate* - a range of values.

A **confidence interval** is an interval estimate which has an associated *confidence level*. The confidence level tells us the probability that the *procedure* that is used to construct the confidence interval will result in the interval containing the true population parameter. It is *not* the probability that the population parameter lies in the range.

This is a very counter-intuitive concept which we shall now illustrate in this exercise.

## Setup

First, create a new `R` script within Rstudio then we will start with some code preamble. We will use the package `ggplot2` for plotting.

```{r}
library(ggplot2)
```

Use `install.packages("ggplot2")` in the console window if the package is not installed on your system.

Lets define the width of an interval, we will set this to 1 initially but we will change this later on:

```{r}
interval_width = 1 # width of confidence interval
```

## Simulating data

We are now going to generate some simulated data for our experiment. We will create 30 samples from a Normal distribution with mean 2.5 and variance 1. These are *true* values of the population parameters. In a real experiment, we would not know these values but using simulated data, we obviously control these.

Let define these first:

```{r}
# number of data points to generate
n <- 30
# population mean
mu <- 2.5
# population standard deviation (square root of population variance)
sigma <- 1.0
```

Now, we generate some normally distributed data using the `R` function `rnorm`,

```{r}
# generate n values from the  Normal distribution N(mu, sigma)
x <- rnorm(n, mean = mu, sd = sigma)
```

We now have 30 samples from a Normal distribution with population mean 2.5 and variance 1.

## Constructing the confidence interval

We are going to pretend that we do not know the population mean value (2.5) used to generate this dataset and try to provide an interval estimate for it from the simulated sample data.

Remember, from lectures, that the sample mean \(\bar{x}\) is a natural point estimate for the population mean \(\mu\).

```{r}
x_bar = mean(x) # compute sample mean
```

so a suitable interval might be centred on the sample mean and extend out,

```{r}
interval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2)
```

Let’s look at this interval:

```{r}
print(interval)
```
> **Q: Does the confidence interval contain the true parameter?**

## Experiment

The previous experiment only examined one simulated dataset so we cannot fully understand the probabilistic interpretation of the confidence interval just yet. At the moment, the interval you have calculated will either contain the population mean or not.

In order to understand the probabilistic interpretation, we will need to generate many data sets, construct confidence intervals as we have for each and then see across all generated data sets, how often those intervals cover the true population mean.

For a Monte Carlo simulation, we will need many repeats of the simulation. Lets define the number of repeats to be used:

```{r}
nreps <- 1000 # number of Monte Carlo simulation runs
```

We will use 1000 simulations initially to make the code quick to run but you may want to make this higher later on for greater accuracy.

Now, let us define a series of interval widths to simultaneously test,

```{r}
# define a series of interval widths
interval_width <- seq(0.1, 1.0, 0.1)
# store the number of interval widths generated
n_interval_widths <- length(interval_width)
```
This creates a sequence of values from 0.1 to 1.0 in steps of 0.1 in the vector `interval_width`:

```{r}
print(interval_width)
```

Now, we will create a vector of zeros of the same length. We will use this to store the number of times that a confidence interval of those specific widths contain the true population mean

```{r}
# create a vector to store the number of times the population mean is contained
mu_contained <- rep(0, n_interval_widths)
```

The hard work now begins. We use a `for` loop to repeat the simulation `nreps` times. Within each loop, we will simulate a new data set, compute a sample mean and then check if the confidence interval contains the true population mean. Since we are using more than one confidence width, we use a second `for` loop to cycle through the different widths.

```{r}
for (replicate in 1:nreps) {

  x <- sigma * rnorm(n) + mu # simulate a data set

  xbar <- mean(x) # compute the sample mean

  # for each interval width that we are testing ...
  for (j in 1:n_interval_widths) {
    # check if the interval contains the true mean
    if ((mu > xbar - 0.5 * interval_width[j]) &
        (mu < xbar + 0.5 * interval_width[j])) {
      # if it is, we increment the count by one for this width
      mu_contained[j] <- mu_contained[j] + 1
    }
  }

}
```

We can now calculate, for each width, an estimate of the probability that a confidence interval of that width will contain the population mean.

```{r}
probability_mean_contained <- mu_contained / nreps
```

Let’s use `ggplot2` to plot this relationship,

```{r}
# create a data frame containing the variables we wish to plot
df <- data.frame(interval_width = interval_width,
                 probability_mean_contained = probability_mean_contained)

# initialise the ggplot
plt <- ggplot(df, aes(x = interval_width, y = probability_mean_contained))
# create a line plot
plt <- plt + geom_line()
# add a horizontal axis label
plt <- plt + xlab("Interval Width")
# create a vertical axis label
plt <- plt + ylab("Probability that mu is contained")

print(plt) # plot to screen
```

Can you see that an interval width of \(0.6\times (\bar{x} \pm 0.3)\) gives a confidence interval close to 90% probability of containing the population mean?

Remember from the lectures that we saw that the theory says \(\bar{x} \pm 1.65\,\frac{\sigma}{\sqrt{n}}\) gives a 90% confidence interval?

So, if we compute  \(2 \pm 1.65\, \frac{\sigma}{\sqrt{n}}\), what do we get?

```{r}
print(2 * 1.65 * sigma / sqrt(n))
```

The Monte Carlo estimate matches up with the theory!

## Problem: Confidence Interval

Can you devise a way to compute a confidence interval for the population variance?

You can make use of the following as a point estimate of the sample variance:

\[
  s^2 = \frac{1}{n - 1}\sum_{i = 1}^n (x - \bar{x})^2
\]

which can be calculated using the `sd` function in `R`.

# Model Answer: Confidence Interval {-}

```{r}
# create a vector to store the number of times
# the population variance is contained
sigma_contained <- rep(0, n_interval_widths)

for (replicate in 1:nreps) {

  x <- rnorm(n, mean = mu, sd = sigma) + mu # simulate a data set

  sigmabar <- sd(x) # compute the sample standard deviation

  # for each interval width that we are testing ...
  for (j in 1:n_interval_widths) {
    # check if the interval contains the true mean
    if ((sigma > sigmabar - 0.5 * interval_width[j]) &
        (sigma < sigmabar + 0.5 * interval_width[j])) {

      # if it is, we increment the count by one for this width
      sigma_contained[j] <- sigma_contained[j] + 1
    }
  }
}

probability_var_contained <- sigma_contained / nreps

# create a data frame containing the variables we wish to plot
df <- data.frame(interval_width = interval_width,
        probability_var_contained = probability_var_contained)

# initialise the ggplot
plt <- ggplot(df, aes(x = interval_width, y = probability_var_contained))
# create a line plot
plt <- plt + geom_line()
# add a horizontal axis label
plt <- plt + xlab("Interval Width")
# create a vertical axis label
plt <- plt + ylab("Probability that sigma is contained")

# plot to screen
print(plt)

print(df)
```

<!--chapter:end:03-confidence-interval.Rmd-->

# Computational Testing Techniques

In this practical we will look at various hypothesis testing problems that explores how we can perform hypothesis testing in R. Please make an attempt before looking at the models answers.

## Problem 1

A process for filling milk cartons is claimed to fill each carton with an average of 260g.

The population fill weight is known to be normal, with a standard deviation of 1.65g.

A random sample of five cartons was collected, and the content weighed, yielding the following (in g.)

```
263.9, 266.2, 266.3, 266.8, 265.0
```

Construct a suitable hypothesis test, at the 1% significance level, to determine whether cartons are being over-filled.

## Problem 2

The mean length of a certain type of component is assumed to be 100mm. Concerns are raised that the mean length is not 100mm.

A random sample of size 45 was obtained, yielding \(\bar{x}=103.11\) and \(s=53.5\).

Perform a hypothesis test, at the 5% level, to determine whether these concerns are justified.

## Problem 3

Can you write your own `z_test` function to perform one-sided or two-sided location (z) tests?

## Problem 4

A manufacturing process yields a product that has a quality control specification of \(\mu_0=5.4\).

A random sample of size \(n=5\) had a sample mean of \(5.64\) and sample variance of \(0.05\).

Conduct a hypothesis test, at the \(5%\) significance level, to assess whether the process is meeting specification.

## Problem 5

Two methods of filling standard gas cylinders are claimed to be different.

In particular, process \(A\) is claimed to yield a higher pressure that process \(B\).

> A random sample of 72 cylinders were filled using process $A$, yielding $\bar{x}_A = 88$ and $s_A^2 = 4.5$.

> A random sample of 48 cylinders were filled with process $B$, yielding $\bar{x}_B = 79$ and $s_B^2 = 4.2$.

Conduct a hypothesis test, at the 5% significance level, to investigate this claim.
## Problem 6

Two catalysts are available for a chemical process. Catalyst B is cheaper than catalyst A.

Provided catalyst B produces the same mean yields, it should be preferred.

To compare methods an experiment was conducted yielding the following data:

> \(A\): 91.50 94.18 92.18 95.39 91.79 89.07 94.72 89.21

> \(B\): 89.19 90.95 90.46 93.21 97.19 97.04 91.07 92.75

Is there evidence to say the two catalysts produce different mean yields? Test at the 5% significance level

## Problem 7

In an animal behaviour experiment a group of 90 rats proceed down a ramp to one of three doors.

The observed counts for each door were:

> Door: 1, 2, 3

> Rats: 23, 36, 31

Is there evidence to suggest a preference for a specific door? Test at the 5% significance level.

## Problem 8

The number of accidents at a junction per week, \(Y\), was observed over a 50 week period, yielding:

> $y$: $0, 1, 2, \geq 3$

> $O$: $32, 12, 6, 0$

Test the hypothesis, at the 1% level, that \(Y \sim \operatorname{Poisson} (\lambda)\)

# Model Answers: Computational Testing {-}

## Problem 1

The data is:

```{r}
x <- c(263.9, 266.2, 266.3, 266.8, 265.0)
```

Now construct some summary statistics and define some given parameters:

```{r}
x_bar <- mean(x) # compute sample mean
sigma <- 1.65 # population standard deviation is given
mu <- 260 # population mean to be tested against
n <- length(x) # number of samples
```
Construct the z-statistic:

```{r}
z <- (x_bar - mu) / (sigma / sqrt(n))
print(z)
```

Check if the z-statistic is in the critical range. First, work out what the z-value at the edge of the critical region is:

```{r}
z_threshold <- qnorm(1 - 0.01, mean = 0, sd = 1)
print(z_threshold)
```

Thus, the z-statistic is much greater than the threshold and there is evidence to suggest the cartons are overfilled.

## Problem 2

Parameters given by the problem:

```{r}
x_bar <- 103.11
s <- 53.5
mu <- 100
n <- 45
```
Compute the z-statistic assuming large sample assumptions apply:

```{r}
z <- ( x_bar - mu )/(s/sqrt(n))
print(z)
```

Now, work out the thresholds of the critical regions:

```{r}
z_upper <- qnorm(1 - 0.025, mean = 0, sd = 1)
print(z_upper)

z_lower <- qnorm(0.025, mean = 0, sd = 1)
print(z_lower)
```

The z-statistic is outside the critical regions and therefore we do not reject the null hypothesis.

## Problem 3

```{r}
z_test <- function(x, mu, popvar){

  one_tail_p <- NULL

  z_score <- round((mean(x) - mu) / (popvar / sqrt(length(x))), 3)

  one_tail_p <- round(pnorm(abs(z_score),lower.tail = FALSE), 3)

  cat(" z =", z_score, "\n",
    "one-tailed probability =", one_tail_p, "\n",
    "two-tailed probability =", 2 * one_tail_p)

  return(list(z = z_score, one_p = one_tail_p, two_p = 2 * one_tail_p))
}

x <- rnorm(10, mean = 0, sd = 1) # generate some artificial data from a N(0, 1)
out <- z_test(x, 0, 1) # null should not be rejected!

print(out)

x <- rnorm(10, mean = 1, sd = 1) # generate some artificial data from a N(1, 1)
out <- z_test(x, 0, 1) # null should be rejected!
print(out)

```

## Problem 4

Define some parameters

```{r}
mu <- 5.4
n <- 5
x_bar <- 5.64
s2 <- 0.05
```
Compute the t-statistic:

```{r}
t <- (x_bar - mu) / sqrt(s2 / n)
print(t)
```

Work out the thresholds of the critical regions:


```{r}
t_upper <- qt(1 - 0.025, df = n - 1)
print(t_upper)

t_lower <- qt(0.025, df = n - 1)
print(t_lower)
```

The t-statistic is outside of the critical regions so we do not reject the null hypothesis.

## Problem 5

Define the parameters:

```{r}
x_bar_a <- 88
s2_a <- 4.5
n_a <- 72
x_bar_b <- 79
s2_b <- 4.2
n_b <- 48
mu_a <- 0
mu_b <- 0
```

Compute the z-statistic:

```{r}
z <- ((x_bar_a - x_bar_b) - (mu_a - mu_b)) / sqrt(s2_a / n_a + s2_b / n_b)
print(z)
```

Work out for the 5% significance level, the critical values:

```{r}
z_upper <- qnorm(1 - 0.05, mean = 0, sd = 1)
print(z_upper)
```
There is evidence to support the claim that process \(A\) yields higher pressurisation.

## Problem 6

```{r}
# Data vectors
x_A <- c(91.50, 94.18, 92.18, 95.39, 91.79, 89.07, 94.72, 89.21)
x_B <- c(89.19, 90.95, 90.46, 93.21, 97.19, 97.04, 91.07, 92.75)

# parameters based on data
x_bar_A <- mean(x_A)
s2_A <- var(x_A)
n_A <- length(x_A)
x_bar_B <- mean(x_B)
s2_B <- var(x_B)
n_B <- length(x_B)
```
Compute the pooled variance estimator:

```{r}
s2_p <- ((n_A - 1) * s2_A + (n_B - 1) * s2_B) / (n_A + n_B - 2)
print(s2_p)
```

Compute the t-statistic:

```{r}
t = ( x_bar_A - x_bar_B ) / sqrt( s2_p*(1/n_A + 1/n_B) )
print(t)
```

Work out the critical values:

```{r}
t_upper <- qt(1 - 0.025, df = n_A + n_B - 2)
print(t_upper)
t_lower <- qt(0.025, df = n_A + n_B - 2)
print(t_lower)
```


Since \(|t|<2.14\) we have no evidence to reject the null hypothesis that the mean yields are equal.

Now, let us use the built-in `t.test` command:

```{r}
  out <- t.test(x = x_A, y = x_B, paired = FALSE, var.equal = TRUE,
    conf.level = 0.95, mu = 0, alternative = "two.sided")
  print(out)
```

The options `paired=FALSE` means this is an unpaired t-test, `var.equal=TRUE` forces the estimated variances to be the same (i.e. we are using a pooled variance estimator) and we are testing at 95% confidence level with an alternative hypothesis that the true difference in means is non-zero.

The p-value for the t-test is between 0 and 1. In this case, the value is around 0.72 which means the hypothesis should not be reject.



## Problem 7

Define parameters:

```{r}
x <- c(23, 36, 31)
p <- c(1 / 3, 1 / 3, 1 / 3)
n <- sum(x)
K <- length(x)
```

Compute the expected counts:

```{r}
Ex = p*n
```

Compute the chi-squared statistic:

```{r}
chi2 <- sum((x - Ex)^2 / Ex)
print(chi2)
```

Compute the critical value form the chi-squared distribution:

```{r}
chi_upper <- qchisq(1 - 0.05, df = K-1)
print(chi_upper)
```

Thus there is no evidence to reject the null hypothesis. The data provides no reason to suggest a preference for a particular door.

Now, we could have done this in `R`:

```{r}
out <- chisq.test(x, p = c(1 / 3, 1 / 3, 1 / 3))
print(out)
```

## Problem 8

```{r}
y <- c( 0, 1, 2 )
x <- c( 32, 12, 6 )
```
You will need the `vcdExtra` package to use the `expand.dft` command:

```{r, eval = FALSE}
install.packages("vcdExtra")
```

The `expand.dft` command allows one to convert the frequency table into a vector of samples:

```{r}
library(vcdExtra)
samples <- expand.dft(data.frame(y,Frequency = x), freq = "Frequency")
```
Now we can use the `fitdistr` function in the `MASS` package to estimate the MLE of the Poisson distribution

```{r}
# loading the MASS package
library(MASS)

# fitting a Poisson distribution using maximum-likelihood
lambda_hat <- fitdistr(samples$y, densfun = 'Poisson')
```
Let just solve this directly using R built in function. First compute the expected probabilities under the Poisson distribution using `dpois` to compute the Poisson pdf:

```{r}
pr <- c(0, 0, 0)
pr[1] <- dpois(0, lambda = lambda_hat$estimate)
pr[2] <- dpois(1, lambda = lambda_hat$estimate)
pr[3] <- 1 - sum(pr[1:2])
```

Then apply `chisq.test`:

```{r}
out <- chisq.test(x, p = pr)

print(out)
```

Actually, in this case the answer is wrong(!), we need to apply an additional loss of degree of freedom to account for the use of the MLE. However, we can re-use values already computed by `chisq.test`:

```{r}
chi2 <- out$statistic
print(chi2)

chi2_lower <- qchisq(1 - 0.01, df = 1)
print(chi2_lower)
```

Hence, there is no evidence to reject the null hypothesis. There is no reason to suppose that the Poisson distribution is not a plausible model for the number of accidents per week at this junction.

<!--chapter:end:04-comp-testing.Rmd-->

# Practical: Linear regression

In this practical you will go through some of the basics of linear modeling in `R` as well as simulating data. The practical contains the following elements:

- simulate linear regression model
- investigate parameters
- characterize prediction accuracy
- correlation of real world data

We will use `reshape2`, `ggplo2`, and `bbmle` packages. Run the following command to make sure they are installed and loaded

```{r eval=FALSE}
install.packages("ggplot2")
install.packages("reshape2")
install.packages("bbmle")
```

```{r}
library(ggplot2)
library(reshape2)
library(bbmle)
```

```{r set, echo = F, warning=F}
library(hrbrthemes)
theme_anas <- theme_set(theme_ipsum_ps())
theme_anas <- theme_update(
  axis.title.x = element_text(size = rel(1.7)),
  axis.title.y = element_text(size = rel(1.7))
)


extrafont::loadfonts(device = "postscript", quiet = TRUE)

knitr::opts_chunk$set(dev = 'png',
                      dpi = 300,
                      dev.args = list(bg = 'transparent'))
```

## Data

For this practical you will require three datasets:

- `stork.txt` ([download](https://canvas.bham.ac.uk/files/8117073/download?download_frd=1))
- `lr_data1.Rdata` ([download](https://raw.github.com/anasrana/module1-practical_Bham/master/data/lr_data1.Rdata))
- `lr_data2.Rdata` ([download](https://raw.github.com/anasrana/module1-practical_Bham/master/data/lr_data2.Rdata)).

## Simulating data

You will simulate data based on the simple linear regression model:

$$
y_i = \beta_0 + \beta_1\, x_i + \epsilon_i,
$$

where $(x_i, y_i)$ represent the $i$-th measurement pair with $i = 1, \ldots, N$, $\beta_0$ and $\beta_1$ are regression coefficients representing intercept and slope respectively. We assume the noise term $\epsilon_i \sim N(0, \sigma^2)$ is normally distributed with zero mean and variance $\sigma^2$.

First we define the values of the parameters of linear regression $(\beta_0, \beta_1, \sigma^2)$:

```{r }
b0 <- 10 # regression coefficient for intercept
b1 <- -8 # regression coefficient for slope
sigma2 <- 0.5 # noise variance
```

In the next step we will simulate $N = 100$ covariates $x_i$ by randomly sampling from the standard normal distribution:

```{r }
set.seed(198) # set a seed to ensure data is reproducible
N <- 100 # no of data points to simulate
x <- rnorm(N, mean = 0, sd = 1) # simulate covariate

```

Next we simulate the error term:

```{r }
# simulate the noise terms, rnorm requires the standard deviation
e <- rnorm(N, mean = 0, sd = sqrt(sigma2))
```

Finally we have all the parameters and variables to simulate the response variable $y$:

```{r }
# compute (simulate) the response variable
y = b0 + b1 * x + e
```

We will plot our data using `ggplot2` so the data need to be in a `data.frame` object:

```{r sim-data}
# Set up the data point
sim_data <- data.frame(x = x, y = y)

# create a new scatter plot using ggplot2
ggplot(sim_data, aes(x = x, y = y)) +
  geom_point()

```

We define the true data `y_true` to be the true linear relationship between the covariate and the response without the noise.

```{r }
# Compute true y values
y_true <- b0 + b1 * x

# Add the data to the existing data frame
sim_data$y_true <- y_true

```

Now we will add the true values of $y$ to the scatter plot:

```{r scatter-true}
lr_plot <- ggplot(sim_data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = y_true), colour = "red")

print(lr_plot)
```

## Fitting simple linear regression model

### Least squared estimation

Now that you have simulated data you can use it to regress $y$ on $x$, since this is simulated data we know the parameters and can make a comparison. In `R` we can use the function `lm()` for this, by default it implements a least squares estimate:

```{r }
# Use the lm function to fit the data
ls_fit <- lm(y ~ x, data = sim_data)

# Display a summary of fit
summary(ls_fit)
```

The output for `lm()` is an object (in this case `ls_fit`) which contains multiple variables. To access them there are some built in functions, e.g. `coef()`, `residuals()`, and `fitted()`. We will explore these in turn:

```{r lm-fit}
# Extract coefficients as a named vector
ls_coef <- coef(ls_fit)

print(ls_coef)

# Extract intercept and slope
b0_hat <- ls_coef[1] # alternative ls_fit$coefficients[1]
b1_hat <- ls_coef[2] # alternative ls_fit$coefficients[2]

# Generate the predicted data based on estimated parameters
y_hat <- b0_hat + b1_hat * x
sim_data$y_hat <- y_hat # add to the existing data frame

# Create scatter plot and lines for the original and fitted
lr_plot <- ggplot(sim_data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = y_true), colour = "red", size = 1.3) +
  # plot predicted relationship in blue
  geom_line(aes(x = x, y = y_hat), colour = "blue")

# force Rstudio to display the plot
  print(lr_plot)
```

The estimated parameters and the plot shows a good correspondence between fitted regression parameters and the true relationship between $y$ and $x$. We can check this by plotting the residuals, this data is stored as the `residuals` parameter in the `ls_fit` object.

```{r resid-scatter}
# Residuals
ls_residual <- residuals(ls_fit) # can also be accessed via ls_fit$residuals

# scatter plot of residuals
plot(ls_residual)
```

A better way of summarising the data is to visualise them as a histogram:

```{r resid_hist}

hist(ls_residual)

```

We expect the mean and variance of the residuals to be close to the level used to generate the data.

```{r }
print(mean(ls_residual))

print(var(ls_residual))
```

This is as expected since subtracting a good fit from the data leaves $\epsilon$ which has $0$ mean and $0.5$ variance.

### Maximum likelihood estimation

Next you will look at maximum likelihood estimation based on the same data you simulated earlier. This is a bit more involved as it requires you to explicitly write the function you wish to minimise. The function we use is part of the `bbmle` package.

```{r warning=FALSE}
# Loading the required package
library(bbmle)

# function that will be minimised. It takes as arguments all parameters
# Here we are helped by the way R works we don't have to explicitly pass x.
# The function will use the existing estimates in the environment
mle_ll <- function(beta0, beta1, sigma) {
  # first we predict the response variable based on the guess for our response
  y_pred = beta0 + beta1 * x

  # next we calculate the normal distribution based on the predicted value
  # the guess for sigma and return the log
  log_lh <- dnorm(y, mean = y_pred, sd = sigma, log = TRUE)

  #  We returnr the negative sum of the log likelihood
  return(-sum(log_lh))
}

# This is the function that actually performs the estimation
# The first variable here is the function we will use
# The second variable passed is a list of initial guesses of parameters
mle_fit <- mle2(mle_ll, start = list(beta0 = -1, beta1 = 20, sigma = 10))

# With the same summary function as above we can output a summary of the fit
summary(mle_fit)
```

The estimated parameters using the maximum likelihood are also a very good estimate of the true values.

## Effect of variance

Now investigate the quality of the predictions further by simulating more data sets and seeing how the variance affects the quality of the fit as indicated by the mean-squared error (mse).

To start you will define some parameter for the simulations, the number of simulations to run for each variance, and the variance values to try.

```{r }
# number of simulations for each noise level
n_simulations <- 100

# A vector of noise levels to try
sigma_v <- c(0.1, 0.4, 1.0, 2.0, 4.0, 6.0, 8.0)
n_sigma <- length(sigma_v)

# Create a matrix to store results
mse_matrix <- matrix(0, nrow = n_simulations, ncol = n_sigma)

# name row and column
rownames(mse_matrix) <- c(1:n_simulations)
colnames(mse_matrix) <- sigma_v
```

Next we will write a nested `for` loop. The first loop will be over the variances and a second loop over the number of repeats. We will simulate the data, perform a fit with `lm()`. We can use the `fitted()` function on the resulting object to extract the fitted values $\hat{y}$ and use this to compute the mean-squared error from the true value $y$.

```{r }
# loop over variance
for (i in 1:n_sigma) {
  sigma2 <- sigma_v[i]

  # for each simulation
  for (it in 1:n_simulations) {

    # Simulate the data
    x <- rnorm(N, mean = 0, sd = 1)
    e <- rnorm(N, mean = 0, sd = sqrt(sigma2))
    y <- b0 + b1 * x + e

    # set up a data frame and run lm()
    sim_data <- data.frame(x = x, y = y)
    lm_fit <- lm(y ~ x, data = sim_data)

    # compute the mean squared error between the fit and the actual y's
    y_hat <- fitted(lm_fit)
    mse_matrix[it, i] <- mean((y_hat - y)^2)

  }
}
```

We created a matrix to store the mse values, but to plot them using `ggplot2` we have to convert them to a `data.frame`. This can be done using the `melt()` function form the `reshape2` library. We can compare the results using boxplots.


```{r effect-plot, fig.width = 12}
library(reshape2)

# convert the matrix into a data frame for ggplot2
mse_df <- melt(mse_matrix)
# rename the columns
names(mse_df) <- c("Simulation", "variance", "MSE")

# now use a boxplot to look at the relationship between
# mean-squared prediction error and variance
mse_plt <- ggplot(mse_df, aes(x = variance, y = MSE)) +
  geom_boxplot(aes(group = variance))

print(mse_plt)
```

You can see that the variances of the mse and the value of the mse go up with increasing variance in the simulation.

What changes do you need to make to the above function to plot the accuracy of the estimated regression coefficients as a function of variance?

## Exercise

### Part I

Read in the data in `stork.txt`, compute the correlation and comment on it.

The data represents `no of storks` (column 1) in Oldenburg Germany from $1930 - 1939$ and the number of people (column 2).

### Part II

Fit a simple linear model to the two data sets supplied (`lr_data1.Rdata` and `lr_data2.Rdata`). In both files the $(x,y)$ data is saved in two vectors, $x$ and $y$.

Download the data from Canvas, you can read it into `R` and plot it with the following commands:

```{r ex-1}
load("lr_data1.Rdata")
plot(x, y)

load("lr_data2.Rdata")
plot(x, y)

```

Fit the linear model and comment on the differences between the data.


### Part III

Investigate how the sample size will affect the quality of the fit using mse, use the code for investigating the affect of variance as inspiration.

<!--chapter:end:06-linear-regression.Rmd-->

# Model answers: Linear regression {-}

## Exercise I

```{r }
library(ggplot2)
library(reshape2)
```

```{r eval=F}
stork_dat <- read.table("stork.txt", hedaer = TRUE)
```

```{r echo=F}
stork_dat <- read.table("data/stork.txt", header = T)
```

```{r }
ggplot(stork_dat, aes(x = no_storks, y = people)) +
  geom_point(size = 2)

```

This is a plot of number of people in Oldenburg (Germany) against the number of storks. We can calculate the correlation in `R`

```{r }
cor(stork_dat$no_storks, stork_dat$peopl)
```

This is a very high correlation, and obviously there is no causation. Think about why there would be a correlation between these two random variables.

## Exercise II

```{r }
# load first data set and create data.frame
load("lr_data1.Rdata")
sim_data1 <- data.frame(x = x, y = y)

# load second data set and create data.frame
load("lr_data2.Rdata")
sim_data2 <- data.frame(x = x, y = y)

lr_fit1 <- lm(y ~ x, data = sim_data1)
lr_fit2 <- lm(y ~ x, data = sim_data2)

```

### Comparison of data

```{r comp-scatter}
ggplot(sim_data1, aes(x = x, y = y)) +
  geom_point(size = 1.5) +
  geom_point(data = sim_data2, color = "red", shape = 18)

```

If we plot the data on top of each other, the first data set in black and the second one in red, we can see a small number of points are different between the two data sets.

```{r }
summary(lr_fit1)

summary(lr_fit2)
```

From the summary data we can see a discrepancy between the two estimates in the regression coefficients ($\approx 1$), though the error in the estimate is quite large. The other thing to notice is that the summary of the residuals look quite different. If we investigate further and plot them we see:

```{r resid}
plot(residuals(lr_fit1))

plot(residuals(lr_fit2))
```

Here we can once again see the outliers in the second data set which affect the estimation. We now plot the histogram and boxplots for comparison:

```{r hist-resid}
hist(residuals(lr_fit1))

hist(residuals(lr_fit2))

boxplot(residuals(lr_fit2), residuals(lr_fit1))
```

Her we can see that the distribution of the residuals has significantly changed in data set 2.

A change in only 4 data points was sufficient to change the regression coefficients.


## Exercise II

```{r }
b0 <- 10 # regression coefficient for intercept
b1 <- -8 # regression coefficient for slope
sigma2 <- 0.5 # noise variance

# number of simulations for each sample size
n_simulations <- 100

# A vector of sample sizes to try
sample_size_v <- c( 5, 20, 40, 80, 100, 150, 200, 300, 500, 750, 1000 )

n_sample_size <- length(sample_size_v)

# Create a matrix to store results
mse_matrix <- matrix(0, nrow = n_simulations, ncol = n_sample_size)

# name row and column
rownames(mse_matrix) <- c(1:n_simulations)
colnames(mse_matrix) <- sample_size_v
```

```{r }
# loop over sample size
for (i in 1:n_sample_size) {
  N <- sample_size_v[i]

  # for each simulation
  for (it in 1:n_simulations) {

    x <- rnorm(N, mean = 0, sd = 1)
    e <- rnorm(N, mean = 0, sd = sqrt(sigma2))
    y <- b0 + b1 * x + e

    # set up a data frame and run lm()
    sim_data <- data.frame(x = x, y = y)
    lm_fit <- lm(y ~ x, data = sim_data)

    # compute the mean squared error between the fit and the actual y's
    y_hat <- fitted(lm_fit)
    mse_matrix[it, i] <- mean((y_hat - y)^2)

  }
}
```

```{r effect-plot-answ, fig.width = 12}
library(reshape2)

mse_df = melt(mse_matrix) # convert the matrix into a data frame for ggplot
names(mse_df) = c("Simulation", "Sample_Size", "MSE") # rename the columns

# now use a boxplot to look at the relationship between mean-squared prediction error and sample size
mse_plt = ggplot(mse_df, aes(x=Sample_Size, y=MSE))
mse_plt = mse_plt + geom_boxplot( aes(group=Sample_Size) )
print(mse_plt)
```

You should see that the variance of the mean-squared error goes down as the sample size goes up and converges towards a limiting value. Larger sample sizes help reduce the variance in our estimators but do not make the estimates more accurate.

Can you do something similar to work out the relationship between how accurate the regression coefficient estimates are as a function of sample size?

<!--chapter:end:07-linear-regression_answers.Rmd-->

# Practical: Principal component analysis

In this practical we will practice some of the ideas outlined in the lecture on Principal Component Analysis (PCA), this will include computing principal components, visualisation techniques and an application to real data.

## Data

For this practical we will use some data that is built into R and we require two additional files you will download:

- `Pollen2014.txt` ([download](https://canvas.bham.ac.uk/files/8117072/download?download_frd=1))
- `SupplementaryLabels.txt` ([download](https://canvas.bham.ac.uk/files/8117074/download?download_frd=1))

## Introduction

We use PCA in order to explore complex datasets. By performing dimensionality reduction we can better visualize the data that has many variables. This technique is probably the most popular tool applied across bioscience problems (e.g. for gene expression problems).

In many real-world dataset we deal with a high dimensional data, e.g. for a number of individuals we can take a number of health related measurement (called variables). This is great, however having a large number of variables also means that it is difficult to plot the data as it is (in its "*raw*" format), and in turn it might be difficult to understand if this dataset contains any interesting patterns/trends/relationships across individuals. Using PCA we visualize such data in a more "*human friendly*" fashion.

Recall:

- PCA performs a linear transformation to data.
- This means that any input data can be visualized in a new coordinate system. The first coordinate (PC 1) variance is found on the first coordinate; each subsequent coordinate is orthogonal to the previous one and contains the larges variance from what was left.
- Each principal component is associated with certain percentage of the total variation in the dataset.
- If variables are strongly correlated with one another, a first few principal components will enable us to visualize the relationships present in any dataset.
- Eigenvectors describe new directions, whereas accompanying eigenvalues tell us how much variance there is in the data in given direction.
- The eigenvector with the highest eigenvalue is called the first principal component. The second highest eigenvalue would correspond to a second principle component and etc.
- There exist a $d$ number of eigenvalues and eigenvectors; $d$ is also equal to the size of the data (number of dimensions).
- For the purpose of visualization we preselect the first $q$ components, where $q < d$.

## Exercise I

There are many datasets built into `R`. Wed will look at the `mtcars` dataset. Type `?mtcars` to get a description of data. Then use `head()` function to have a look at the first few rows; and `dim()` to get the dimensions of the data.

```{r mtcars-data}
library(ggplot2)
head(mtcars)


dim(mtcars)
```

```{r setpca, echo = F, warning=F}
library(hrbrthemes)
theme_anas <- theme_set(theme_ipsum_ps())
theme_anas <- theme_update(
  axis.title.x = element_text(size = rel(1.7)),
  axis.title.y = element_text(size = rel(1.7))
)
extrafont::loadfonts(quiet=TRUE)

```

In this case we have $32$ examples (cars in this case), and $11$ features.
Now we can perform a principal component analysis, in `R` it is implemented as the `prcomp()` function. We can type `?prcomp` to see a description of the function and some help on possible arguments. Here we set `center` and `scale` arguments to `TRUE`, recall from the lecture why this is important. We can try to perform PCA without scaling and centering and compare.

```{r pca}
cars_pca <- prcomp(mtcars, center = TRUE, scale = TRUE)

```

We can use the `summary()` function to summarise the results from PCA, it will return the standard deviation, the proportion of variance explained by each principal component, and the cumulative proportion.

```{r pca-sum}
pca_summary <- summary(cars_pca)
print(pca_summary)
```

*Note,* `Proportion of Variance` will always add up to $1$. Here the `PC1` explain $60.08%$ of the variance, and `PC2` explains $24.09%$, which means together `PC1` and `PC2` account for $84.17%$  of the variance.

Using the `str()` function we can see the full structure of an `R` object, or alternatively using `?prcomp` in the *Value* section. In this case the `cars_pca` variable is a list containing several variables, `x` is the data represented using the new principal components. We can now plot the data in the first two principal components:

```{r plot_2d, fig.width=10, fig.height=10}

pca_df <- data.frame(cars_pca$x, make = stringr::word(rownames(mtcars), 1))

ggplot(pca_df, aes(x = PC1, y = PC2, col = make)) +
geom_point(size = 3) +
labs(x = "PC1 60.08%",
     y = "PC2 24.09 %",
     title = "Principal components for mtcars") +
theme(legend.position = "bottom")

```

Here we added a color based on the make of each car. We can observe which samples (or cars) cluster together. Have a look at these variables and decide why certain cars or models would cluster together.

We created this plot using the `ggplot2` package, it is also possible to do this using base plot if you prefer.

```{r pca_plot-base, eval=FALSE}
plot(pca_df$PC1, pca_df$PC2)
```

## Exercise II

Next we look at another representation of the data, the *biplot*. This is a combination of a PCA plot of the data and a *score plot*. We saw the PCA plot in the previous section in a *biplot* we add the original axis as arrows.

```{r biplot, fig.width=10, fig.height=10}
biplot(cars_pca)
```

We can see the original axis starting from the origin. Therefore we can make observations about the original variables (e.g. `cyl` and `mpg` contribute to PC1) and how the data points relates to these axes.

## Exercise III

Now try to perform a PCA on the `USArrests` data also build into `R`. Typing `?USArrests` will give you further information on the data. Perform the analysis on the subset `USArrests[, -3]` data.

## Exercise IV: Single cell data

We can now try to apply what we learned above on a more realistic datasets. You can download the data either on *canvas* or using these links [`Pollen2014.txt`](https://raw.github.com/anasrana/module1-practical_Bham/master/data/Pollen2014.txt) and [`SupplementaryLabels.txt`](https://raw.github.com/anasrana/module1-practical_Bham/master/data/SupplementaryLabels.txt). Her we will be dealing with single cell RNA-Seq (scRNA-Seq) data, which consist of $300$ single cells measured across $8686$ genes.

```{r sc-data}

pollen_df <-read.table("Pollen2014.txt", sep=',', header = T,row.names=1)

label_df <-read.table("SupplementaryLabels.txt", sep=',', header = T)

pollen_df[1:10, 1:6]

dim(pollen_df)

```

Measurements of scRNA-Seq data are integer counts, this data does not have good properties so we perform a transformation on the data. The most commonly used transformation on RNA-Seq count data is $\log_2$. We will also transpose the data matrix to rows representing cells and columns representing genes. This is the data we can use to perform PCA.

```{r sc_data}
# scRNA-Seq data transformation
pollen_mat <- log2(as.matrix(pollen_df) + 1)
# transpose the data
pollen_mat <- t(pollen_mat)

```

We will now use information that we read into the `label_df` variable to rename cells.

```{r }
# Check which columns we have available
colnames(label_df)

# rename rows
rownames(pollen_mat) <- label_df$Cell_names

```

Next we perform PCA on the data and extract the proportion of variance explained by each component.

```{r pca-sc}
sc_pca <- prcomp(pollen_mat)

# variance is the square of the standard deviation
pr_var <- sc_pca$sdev^2

# compute the variance explained by each principal component
prop_var_exp <- pr_var / sum(pr_var)

```

Think about the calculation and what exactly it means. We can visualise this

```{r pca-scree, fig.width=10, fig.height=6}

var_exp <- data.frame(variance = prop_var_exp, pc = 1:length(prop_var_exp))

ggplot(var_exp[1:30, ], aes(x = pc, y = variance)) +
    geom_bar(stat = "identity") +
    labs(x = "Principal Component",
         y  = "Variance explained")


```

We see that the first few principal components explain significant variance, but after about the PC10, there is very limited contribution. Next we will plot the data using the first two Principal components as before.

```{r sc-pca_plot, fig.width=10, fig.height=12}

sc_pca_df <- data.frame(sc_pca$x, cell = rownames(sc_pca$x),
                        var_exp = prop_var_exp)

ggplot(sc_pca_df, aes(x = PC1, y = PC2, col = cell)) +
    geom_point(size = 2) +
    theme(legend.position = "bottom")

```

Why is it not useful to create biplot for this example?

<!--chapter:end:08-pca.Rmd-->

# Practical: Multiple regression

Previously we have only considered simple linear regression with one response variable and one feature. In this practical we will go through examples with multiple features:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon $$

For this practical we will use data that is already inbuilt in R or is part of the `MASS` package. The only thing we need to do to make the data available is load the `MASS` package.

<!-- ```{r init}
# We load the MAASS package for plotting
library(MASS)

# We load the ggplot2 package for plotting
library(ggplot2)

``` -->

```{r setmr, echo = F, warning=F}
library(ggplot2)
library(hrbrthemes)
theme_anas <- theme_set(theme_ipsum_ps())
theme_anas <- theme_update(
  axis.title.x = element_text(size = rel(1.7)),
  axis.title.y = element_text(size = rel(1.7))
)
extrafont::loadfonts(quiet=TRUE)

```

## Multiple regression

For this part we will use the inbuilt `trees` dataset containing `Volume`, `Girth` and `Height` data for 31 trees.

First we revisit linear regression on this example, recall the function to fit a linear model `lm()`. Consider `Volume` to be the response variable and `Girth` to be the covariate.

```{r }
lr_fit <- lm(Volume ~ Girth, data = trees)
summary(lr_fit)

```

We will now consider a linear regression example with multiple covariates, `Girth` as well as `Height`. In this case of course we know that they are related so we do expect both covariates to be significant.

```{r }
mr_fit <- lm(Volume ~ Girth + Height, data = trees)


summary(mr_fit)
```

*Note*, in the formula you only enter the covariates and not the regression coefficients or any information regarding the noise.

Let us now look at RSS values, we can calculate the RSS for the `lf_fit` object by using `sum(residuals(lr_fit)^2)`. We see that the RSS for LR = `r sum(residuals(lr_fit)^2) %>% round(digits = 2)` and the RSS for MR = `r sum(residuals(mr_fit)^2) %>% round(digits = 2)`. Therefore the fit has improved but the regression coefficient for `Height` is very small and not significant.

One reason for this is that the in the relationship between `Volume`, `Girth`, and `Height` is not additive but rather `Girth` and `Height` are multiplied. Using the fact that $\log(a*b) = \log(a) + \log(b)$ we can consider the log-transformed data in a linear model.

```{r }
mrl_fit <- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)

summary(mrl_fit)
```

Now we see that the regression coefficient is large and both covariates are significant. This shows that we need to ensure we understand the relationship between covariates before we construct our model.

## Categorical covariates

Recall from the lecture that covariates don't need to be numerical but can also be *categorical*. We will now explore regression with a categorical variable. Load a new dataset which is included in the `MASS` package, you won't be able to load this dataset if package isn't installed. Load the dataset explore what the data looks like.

```{r bwt-data}
library(MASS)
data("birthwt")

head(birthwt)

summary(birthwt)
```

We will give the data more interpretable names and generally cleanup the data a little bit.

```{r bplt, fig.width = 8, fig.height = 7}
# rename columns
colnames(birthwt) <- c("bwt_below_2500", "mother_age", "mother_weight", "race",
                       "mother_smokes", "previous_prem_labor", "hypertension",
                       "uterine_irr", "physician_visits", "bwt_grams")

birthwt$race <- factor(c("white", "black", "other")[birthwt$race])
birthwt$mother_smokes <- factor(c("No", "Yes")[birthwt$mother_smokes + 1])
birthwt$uterine_irr <- factor(c("No", "Yes")[birthwt$uterine_irr + 1])
birthwt$hypertension <- factor(c("No", "Yes")[birthwt$hypertension + 1])

ggplot(birthwt, aes(x = mother_smokes, y = bwt_grams)) +
    geom_boxplot() +
    labs(title = "Data on baby births in Springfield (1986)",
         x = "Does the mother smoke?",
         y = "Birth-weight [grams]")

```

```{r bwt_sct, fig.width=10, fig.height=8}
ggplot(birthwt, aes(x = mother_age, y = bwt_grams, col = mother_smokes)) +
    geom_point()
```

Now we perform linear regression using the categorical variable, it is no different than performing linear regression on numeric data. The difference is in interpretation.

```{r }
bwt_fit <- lm(bwt_grams ~ mother_smokes, data = birthwt)

summary(bwt_fit)
```

When you put a categorical variable in the formula for `lm` as in this case `bwt_grams ~ mother_smokes` where we have two levels in the categorical variable. If we consider this model as $y = \beta_0 + \beta_1 x + \epsilon$
The coefficients in the model can be interpreted as follows:

- $\beta_0$ is average birth weight where the mother was a non smoker
- $\beta_0 + \beta_1$ is the average birth weight where the mother is a smoker
- $\beta_1$ is the average difference in birth weight for babies between mother that were smokers and mothers that were non smokers.

Categorical variables can also have more than two levels and in those cases each additional level can be interpreted in the same way.

## Residuals

Recall from the lectures the residuals are the differences between the observed data $y$ and the fitted values $\hat{y}$. One of the assumptions we make in the simple linear regression model is that the residuals should be normally distributed. To extract residuals from an `lm` object we will use the `residuals()` function.

```{r bwt_resid, fig.width = 10, fig.height = 5}

residuals_df <-
data.frame(resid = residuals(bwt_fit))

ggplot(residuals_df, aes(x = resid)) +
    geom_histogram(bins = 10)

```

Even if we consider that these residuals look like they are normally distributed we need to get better understanding of this we will use the Q-Q Plot. You can take a look at the wiki to get a better understanding ([Q–Q plot - Wikipedia](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot)). In simple terms if the residuals are normally distributed we expect them to be on the diagonal straight line on a Q-Q plot. The simplest way to get such a plot is using the `plot()` function and specifically for an `lm` object it has an option `which = ` that takes a numeric value depending on which plot you want to plot.

```{r qq, fig.width = 6, fig.height = 6}
plot(bwt_fit, which = 2)
```

As we can see in this example the residuals are very close to normal with some outliers especially towards larger values of the residual. This would indicate that the model as it stands does not fulfill that assumption fully but comes close.


## Gradient descent algorithm (+)

Finally, in todays practical we will implement the *gradient descent algorithm* which we discussed in the lecture.

For simplicity we will only consider the case with one covariate. In this section we will use simulated data and compare the results with `lm()`. The model we will simulate from is:

$$y = 2 + 3 x + \epsilon$$

```{r sim-mr}
# setting seed to be able to reproduce the simulation
set.seed(200)

# number of samples
n_sample <- 1000

# We sample x values from a uniform distribution in the range [-5, 5]
x <- runif(n_sample, -5, 5)
# Next we compute y
y <- 3 * x + 2 + rnorm(n = n_sample, mean = 0, sd = 1)

sim_df <- data.frame(x = x, y = y)

ggplot(sim_df, aes(x = x, y = y)) +
    geom_point()
```

Recall that in gradient descent we want to minimise the Mean Squared Error ($J(\beta)$) which is the cost function. The first step is to write this cost function in R. For simplicity we will use matrix multiplication, which in R is implemented as `%*%`. (*Note*, to get help on these function with special characters you can't simply run the command `?%*%` instead you have to put it in quotes `?"%*%"`.)

```{r cost}
cost_fn <- function(X, y, coef) {
    sum( (X %*% coef - y)^2 ) / (2*length(y))
}
```

To perform an optimisation we will have to initialise parameters, in general optimisation algorithms won't always produce the same results for all choices of initialisations.

```{r init}
# First we set alpha and the number of iterations we will perform
alpha <- 0.2
num_iters <- 100

# next we will initialise regression coefficients
coef <- matrix(c(0,0), nrow=2)
X <- cbind(1, matrix(x))
res <- vector("list", num_iters)

```

We now write a for loop to compute the optimisation, where we store the full history of the opmtimisation.

```{r gd_optim}

for (i in 1:num_iters) {
  error <- (X %*% coef - y)
  delta <- t(X) %*% error / length(y)
  coef <- coef - alpha * delta
  res_df <- data.frame(itr = i , cost = cost_fn(X, y, coef),
                   b0 = coef[1], b1 = coef[2])

  res[[i]] <- res_df
}

```
We created a list to store results `res` it is possible to combine all results into a simple `data.frame` using the `bind_rows()` function from the `dplyr` package. If we look at the final values in the resulting variable we will

```{r , message=FALSE}
library(dplyr)
res_df <- bind_rows(res)
tail(res_df)

```

We can see that $\beta_0 = 2$ and $\beta_1 = 3$ are reproduced faithfully. There are a few ways to visualise the optimisation. We can look at the convergence of the parameters, the cost function itself or even the estimated $y$ at each step of the optimisation.

```{r vis_gd, fig.width = 10, fig.height = 6}

ggplot(res_df, aes(x = itr, y = b1)) +
    geom_line() +
    labs(x = "Iteration",
         y = "Estimated beta_1",
         title = "Visuaslisation of the cconvergence of the beta_1 parameter")


ggplot(res_df, aes(x = itr, y = cost)) +
    geom_line() +
    labs(x = "Iteration",
         y = "Cost function",
         title = "History of cost function at each iteration")

ggplot(sim_df, aes(x = x, y = y)) +
    geom_point(color = "red", alpha = 0.3) +
    geom_abline(data = res_df, aes(intercept = b0, slope = b1),
                alpha = 0.3, col = "darkgreen", size = 0.5) +
    labs(x = "x", y = "y",
         title = "Estimated response at each step during optimisation")

```

Now compare these results to the ones obtained by fitting a linear model in R using the function `lm()`, how different are the results. Try to reproduce these plots with $\alpha =$ (0.02, 0.1, 0.5), and different number of iterations in the optimisation and compare the estimated $\hat{\beta}_0$, and $\hat{\beta}_1$ to the values you use during the simulation step. This will give you an idea how important the right choice of these two parameters is.

<!--chapter:end:09-multiple.Rmd-->

# Practical: Generalised linear models

In a genome-wide association study, we perform an experiment where we select $n$ individuals with a disease (cases) and $n$ individuals without the diseases (controls) and look for genetic differences between these two groups. In particular, we are interested in specific genetic variants (SNPs) that might induce some predisposition towards the disease.

Suppose I observe the following genotypes for a SNP in 4,000 individuals (2,000 cases, 2,000 controls):

- Genotypes: `AA` `Aa` `aa`
- Controls: `3 209 1788`
- Cases: `83 621 1296`

The cases seem to have relatively more `A` alleles than the controls. This might make us suspect that having `A` alleles at this SNP is associated with the disease.

## Data

For this practical we will use two files you can use these links to download them:

- `gwas-cc-ex1.Rdata` ([download](https://raw.github.com/anasrana/module1-practical_Bham/master/data/gwas-cc-ex1.Rdata))
- `gwas-cc-ex2.Rdata` ([download](https://raw.github.com/anasrana/module1-practical_Bham/master/data/gwas-cc-ex2.Rdata))
- `nb_data.Rdata` ([download](https://raw.github.com/anasrana/module1-practical_Bham/master/data/nb_data.Rdata))

## Detecting SNP associations

We have seen in lectures that we can do statistical tests for this type of contingency table using Chi Squared Tests. Let's load example data set and and prepare

```{r }
library(ggplot2) # for plots later
load("gwas-cc-ex1.Rdata")

# how many individuals are there
n <- length(y)
# How many SNPs do we have data for
p <- nrow(X)

# samples that are controls are encoded as 0 in y
control <- which(y == 0)
# disease cases are encoded as 1 in y
cases <- which(y == 1)

```

Now we need to write a loop that scans through all, $p$, SNPs:

```{r pval_loop, warning=F}
# create a vector where p-values will be stored
p_vals <- rep(0, p)

# Loop over SNPs
for (i_p in 1:p) {
    # 1. obtain genotype counts
    counts <- matrix(0, nrow = 2, ncol = 3)
    counts[1, ] <- c(sum(X[i_p, control] == 0),
                     sum(X[i_p, control] == 1),
                     sum(X[i_p, control] == 2))

    counts[2, ] <- c(sum(X[i_p, cases] == 0),
                     sum(X[i_p, cases] == 1),
                     sum(X[i_p, cases] == 2))

    # 2. expected probability of AA
    # (assuming no dependence on case/control status)
    expected_pr_AA <- sum(counts[, 1]) / n
    # expected probability of Aa
    expected_pr_Aa <- sum(counts[, 2]) / n
    # expected probability of aa
    expected_pr_aa <- sum(counts[, 3]) / n

    expected_probs <- c(expected_pr_AA, expected_pr_Aa, expected_pr_aa )

    # 3. do my chi-squared test
    out <- chisq.test(counts, p = expected_probs)
    # extract p value of test and store
    p_vals[i_p] <- out$p.value
}

```

We went through each SNP (rows in matrix `X`), extracted the counts of each genotype (marked `1.` in code) for cases and controls, then we compute expected probability (marked `2.` in code). Finally, we perform a chi-squared contingency table test comparing those observed counts to expected probabilities assuming that genotype is not related to disease status (marked `3.` in code).

```{r setlgm, echo = F, warning=F}
library(hrbrthemes)
theme_anas <- theme_set(theme_ipsum_ps())
theme_anas <- theme_update(
  axis.title.x = element_text(size = rel(1.7)),
  axis.title.y = element_text(size = rel(1.7))
)
extrafont::loadfonts(quiet=TRUE)

```

```{r plot, fig.width=10, fig.height=5, warning=F}
p_val_df <- data.frame(p_val = p_vals, idx = 1:p)

ggplot(p_val_df, aes(x = idx, y = -log10(p_val))) +
    geom_point(size = 2.5, col = "dodgerblue1")

```

This plot is knows as a *Manhattan* plot. One SNP (`i_p = 250`) will pop out as being highly associated with the disease process. Look at the genotype counts (or MAF) for this SNP in the cases and controls to see for yourself that there is large difference in the distribution of genotypes (or MAF).

```{r snp, fig.width=8, fig.height=3.5, warning=F}
i_p <- 250
counts_v <- c(sum(X[i_p, control] == 0), sum(X[i_p, control] == 1),
              sum(X[i_p, control] == 2), sum(X[i_p, cases] == 0),
              sum(X[i_p, cases] == 1), sum(X[i_p, cases] == 2))

snp_procs <- data.frame(counts_v, type = rep(c("control", "cases"), each = 3),
           genotype = rep(c("AA", "Aa", "aa"), 2))

ggplot(snp_procs, aes(x = genotype, y = counts_v, fill = type)) +
    geom_bar(stat = "identity", position = "dodge")
```

## GWAS and logistic regression

Now lets approach this problem using Generalised Linear Models. Lets load a data set containing genotypes in `X` and case-control status in `y`:

```{r gwas2}
# load an example data set (genotypes in X, case-control (1/0) status in y)
load("gwas-cc-ex2.Rdata")

n <- length(y) # how many individuals do we have in total?
p <- nrow(X) # how many SNPs do I have data for?
```

For each of the `p` SNPs we are going to call the `R` GLM function `glm` using the `binomial` family option with the `logit` link function because my outcomes are binary. We will then extract the p-value associated with the regression coefficient for the genotype. This is obtained from applying a hypothesis test (the *Wald Test*) on whether the coefficient has a null value zero.

```{r }
p_vals <- rep(0, p)
for ( j in 1:p ) {
  snp_data <- data.frame(y = y, x = X[j, ])
  glm.fit <- glm(y ~ x, family = binomial(link = logit), data = snp_data  )
  p_vals[j] <- summary(glm.fit)$coefficients[2,4]
}
```
We are testing 1,000 SNPs so lets use Bonferroni correction to adjust these p-values to take into account multiple testing:

```{r }
adj_p_vals <- p.adjust(p_vals, "bonferroni")
```
Lets use the adjusted -log10 p-values to plot a Manhattan plot:

```{r gwas2_plot}
# create data.frame with p-values for plotting with ggplot
p_val_df <- data.frame(p_val = adj_p_vals, idx = 1:p)

ggplot(p_val_df, aes(x = idx, y = -log10(p_val))) +
    geom_point(size = 2.5, col = "dodgerblue1") +
    labs(y = "-log10(adjusted p-value)")

```

You should see a single SNP showing a strong association with disease status.

## Negative binomial and Poisson regression

Molecular biologists study the behavior of protein expression in normal and cancerous tissues. The hypothesis is that the total number of over-expressed proteins depends on the histopathological-derived tumor subtype and an immune cell contexture measure.

You are provided with data on 314 tumours in the file `nb_data.Rdata`. The file contains one `data frame` with the following variables:

- **`overexpressed_proteins`**: response variable of interest.
- **`immunoscore`**: gives a standardized measure of immune cell contexture.
- **`tumor_subtype`**: three-level nominal variable indicating the histopathological sub-type of the tumour. The three levels are Unstable, Stable, and Complex

Let’s load some prerequisite R libraries and the data to produce some summary statistics (*install if required using `install.package()` command* ):

```{r nb-pr1}
# required libraries
library(MASS)
library(foreign)


load("nb_data.Rdata")

# print summary statistics to Console
summary(dat)
```

### Count-based GLMs

The `overexpressed_proteins` measurements are counts. This implies we should use a Poisson based GLM.

In Poisson regression models, the conditional variance is by definition equal to the conditional mean. This can be limiting.

*Negative binomial regression* can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.

It can be considered as a generalization of Poisson regression since it has the same mean structure as Poisson regression but it has an extra parameter to model the over-dispersion. If the conditional distribution of the outcome variable is over-dispersed, the confidence intervals for the Poisson regression are likely to be narrower as compared to those from a Negative Binomial regression model.

In the following we will try both models to see which fits best.

### Fitting a GLM

Below we use the `glm.nb` function from the `MASS` package to estimate a negative binomial regression. The use of the function is similar to that of `lm` for linear models but with the additional requirement of a link function. As count data is always positive, a log link function is useful here.

```{r glm-nb}
glm_1 <- glm.nb(overexpressed_proteins ~ immunoscore + tumor_subtype + gender, data = dat, link=log)

# print summary statistics of glm.nb output object to Console
summary(glm_1)
```

R first displays the call and the deviance residuals. Next, we see the regression coefficients for each of the variables, along with standard errors, z-scores, and p-values. The variable `immunoscore` has a coefficient of -0.006, which is statistically significant at the 5% level (`Pr(>|z|) = 0.0124*`). This means that for each one-unit increase in `immunoscore`, the expected log count of the number of `overexpressed_proteins` decreases by 0.006.

The indicator variable shown as `tumor_subtypeUnstable` is the expected difference in log count between group Unstable and the reference group (`tumor_subtype=Complex`). The expected log count for the Unstable type is approximately 0.4 lower than the expected log count for the Complex type.

The indicator variable for Stable type is the expected difference in log count between the Stable type and the reference Complex group. The expected log count for Stable is approximately 1.2 lower than the expected log count for the Complex type.

### Comparing nested models

To determine if `tumor_subtype` itself, overall, is statistically significant, we can compare a model with and without `tumor_subtype`. The reason it is important to fit separate models is that, unless we do, the overdispersion parameter is held constant and it would not be a fair comparison.

```{r ovdisp}
glm_2 <- glm.nb(overexpressed_proteins ~ immunoscore + gender, data = dat, link = log)

```

We use the `anova` function to compare models using a likelihood ratio test (LRT):

```{r glm_anova}
anova(glm_1, glm_2, test = "LRT")
```

The two degree-of-freedom chi-square test indicates that `tumor_subtype` is a statistically significant predictor of `overexpressed_proteins` (`Pr(Chi) = 3.133546e-10`).

The `anova` function performs a form of *LRT*. It computes the likelihood of the data under the two models being compared and then uses the ration of these likelihood values as a test statistic.

Theory tells us that, for large samples sizes, the (2x) log likelihood ratio has a chi-squared distribution with degrees of freedom equal to the difference in the number of free parameters between the two models being compared. The LRT only applies to *nested models*, i.e. a pair of models where one is a less complex subset of the other.

## Negative-Binomial vs Poisson GLMs

Negative binomial models assume the conditional means are not equal to the conditional variances. This inequality is captured by estimating a dispersion parameter (not shown in the output) that is held constant in a Poisson model. Thus, the Poisson model is actually nested in the negative binomial model. We can then use a likelihood ratio test to compare these two models.

To do this, we will first fit a GLM Poisson regression:

```{r glm_pois}
glm_3 <- glm(overexpressed_proteins ~ immunoscore + tumor_subtype + gender, family = "poisson", data = dat)

```

Now, lets do our likelihood ratio test, we can extract the log-likelihood using `logLik()` and then use `pchisq()` to extract the probability of getting a statistic at least as extreme as this:

```{r loglik}

pchisq(2 * (logLik(glm_1) - logLik(glm_3)), df = 1, lower.tail = FALSE)

```

Note that the more complex model goes first because more complex models always have the larger likelihood.

In this example the associated chi-squared value estimated from `2*(logLik(m1) – logLik(m3))` is around 900 with one degree of freedom. This strongly suggests the negative binomial model, estimating the dispersion parameter, is more appropriate than the Poisson model.

## Further understanding the model (**OPTIONAL**)

For assistance in further understanding the model, we can look at predicted counts for various levels of our predictors. Below we create new datasets with values of `immunoscore` and `tumor_subtype` and then use the predict command to calculate the predicted number of overexpressed proteins

First, we can look at predicted counts for each value of `tumor_subtype` while holding `immunoscore` at its mean. To do this, we create a new dataset with the combinations of `tumor_subtype` and `immunoscore` for which we would like to find predicted values, then use the `predict()` command.

```{r glm_pred}
newdata_1 <-
data.frame(
    immunoscore = mean(dat$immunoscore),
    tumor_subtype = factor(c("Complex", "Unstable", "Stable"), labels = levels(dat$tumor_subtype)),
    gender="male")

newdata_2 <-
data.frame(
    immunoscore = mean(dat$immunoscore),
    tumor_subtype = factor(c("Complex", "Unstable", "Stable"), labels = levels(dat$tumor_subtype)),
    gender="female")

new_data <- rbind(newdata_1, newdata_2)

new_data$phat <- predict(glm_1, new_data, type = "response")

print(new_data)
```


```{r new}
newdata_3 <-
data.frame(
    immunoscore = rep(seq(from = min(dat$immunoscore), to = max(dat$immunoscore), length.out = 100), 3),
    tumor_subtype = rep(factor(c("Complex", "Unstable", "Stable"), labels = levels(dat$tumor_subtype)), each=100),
    gender="male")

newdata_4 <-
data.frame(
    immunoscore = rep(seq(from = min(dat$immunoscore), to = max(dat$immunoscore), length.out = 100), 3),
    tumor_subtype = rep(factor(c("Complex", "Unstable", "Stable"), labels = levels(dat$tumor_subtype)), each=100),
    gender="female")

new_data <- rbind(newdata_3, newdata_4)

new_data <- cbind(new_data, predict(glm_1, new_data, type = "link", se.fit=TRUE))

new_data <- within(new_data, {
  overexpressed_proteins <- exp(fit)
  LL <- exp(fit - 1.96 * se.fit)
  UL <- exp(fit + 1.96 * se.fit)
})

```

```{r glm-plot, fig.width = 12, fig.height = 8}
library(ggplot2)

ggplot(new_data, aes(immunoscore, overexpressed_proteins)) +
    geom_ribbon(aes(ymin = LL, ymax = UL, fill = tumor_subtype), alpha = 0.2) +
    geom_line(aes(colour = tumor_subtype), size = 1.5) +
    labs(x = "Immunoscore",
         y = "Overexpressed Proteins") +
    facet_wrap(~ gender)

```

<!--chapter:end:10-glms.Rmd-->

